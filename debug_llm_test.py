#!/usr/bin/env python3
"""
Debug LLM Test - LLMåˆæœŸåŒ–å•é¡Œã®ãƒ‡ãƒãƒƒã‚°
"""

import asyncio
import sys
import os
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’æœ€åˆã«èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(__file__))

async def debug_llm():
    print("ğŸ” LLMåˆæœŸåŒ–ãƒ‡ãƒãƒƒã‚°ãƒ†ã‚¹ãƒˆ")
    print("=" * 40)

    # ç’°å¢ƒå¤‰æ•°ç¢ºèª
    print("ğŸ”‘ ç’°å¢ƒå¤‰æ•°ç¢ºèª:")
    openai_key = os.getenv("OPENAI_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")

    print(f"   OpenAI API Key: {'âœ… ã‚ã‚Š' if openai_key else 'âŒ ãªã—'}")
    print(f"   Anthropic API Key: {'âœ… ã‚ã‚Š' if anthropic_key else 'âŒ ãªã—'}")

    # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèª
    print("\nğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèª:")
    try:
        import openai
        print("   OpenAI: âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
    except ImportError:
        print("   OpenAI: âŒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—")

    try:
        import anthropic
        print("   Anthropic: âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
    except ImportError:
        print("   Anthropic: âŒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—")

    try:
        import ollama
        print("   Ollama: âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
    except ImportError:
        print("   Ollama: âŒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—")

    # HybridLLMåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
    print("\nğŸ§  HybridLLMåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ:")
    try:
        from src.llm.hybrid_llm import HybridLLM
        llm = HybridLLM()

        print("   HybridLLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ: âœ…")

        # åˆæœŸåŒ–å®Ÿè¡Œ
        await llm.initialize()
        print("   åˆæœŸåŒ–: âœ… æˆåŠŸ")

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        status = await llm.get_status()
        print(f"   ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {status.get('primary_provider')}")
        print(f"   åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {list(status.get('providers', {}).keys())}")

        # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è©³ç´°
        providers = status.get('providers', {})
        for name, provider_status in providers.items():
            available = provider_status.get('available', False)
            print(f"   {name}: {'âœ…' if available else 'âŒ'}")

        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
        if any(p.get('available') for p in providers.values()):
            print("\nğŸ’¬ ç°¡å˜ãªãƒ†ã‚¹ãƒˆ:")
            response = await llm.generate_response("Hello, say 'test successful' if you can read this.")
            print(f"   å¿œç­”: {response}")

        await llm.cleanup()

    except Exception as e:
        print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(debug_llm())