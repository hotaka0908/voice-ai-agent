"""
Hybrid LLM - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMã‚·ã‚¹ãƒ†ãƒ 

Claudeã¨Ollamaã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã¨æ€§èƒ½ã‚’ä¸¡ç«‹ã™ã‚‹LLMã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import json
import asyncio
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
from loguru import logger

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("Anthropic library not available")

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("Ollama library not available")

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI library not available")


class LLMProvider:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, name: str):
        self.name = name
        self.is_available = False

    async def initialize(self, config: Dict[str, Any]):
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        pass

    async def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        raise NotImplementedError

    async def is_healthy(self) -> bool:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        return self.is_available

    def get_status(self) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®çŠ¶æ…‹å–å¾—"""
        return {
            "name": self.name,
            "available": self.is_available
        }


class ClaudeProvider(LLMProvider):
    """Claude (Anthropic) ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self):
        super().__init__("claude")
        self.client = None
        self.model = "claude-3-haiku-20240307"

    async def initialize(self, config: Dict[str, Any]):
        try:
            api_key = config.get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")
            if not api_key or not ANTHROPIC_AVAILABLE:
                logger.warning("Claude API key not available or library missing")
                return

            self.client = anthropic.Anthropic(api_key=api_key)
            self.model = config.get("model", self.model)

            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            await self._test_connection()
            self.is_available = True
            logger.info(f"Claude provider initialized with model: {self.model}")

        except Exception as e:
            logger.error(f"Failed to initialize Claude provider: {e}")
            self.is_available = False

    async def _test_connection(self):
        """æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            response = self.client.messages.create(
                model=self.model,
                max_tokens=10,
                messages=[{"role": "user", "content": "Hello"}]
            )
            logger.debug("Claude connection test successful")
        except Exception as e:
            logger.error(f"Claude connection test failed: {e}")
            raise

    async def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        if not self.is_available or not self.client:
            raise RuntimeError("Claude provider not available")

        try:
            # Anthropicãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
            anthropic_messages = self._convert_messages(messages)

            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            response = self.client.messages.create(
                model=self.model,
                max_tokens=kwargs.get("max_tokens", 1000),
                temperature=kwargs.get("temperature", 0.7),
                messages=anthropic_messages
            )

            return {
                "content": response.content[0].text,
                "model": self.model,
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens
                }
            }

        except Exception as e:
            logger.error(f"Claude generation failed: {e}")
            raise

    def _convert_messages(self, messages: List[Dict]) -> List[Dict]:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’Anthropicå½¢å¼ã«å¤‰æ›"""
        anthropic_messages = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯æœ€åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«çµåˆ
            if role == "system":
                if anthropic_messages and anthropic_messages[0]["role"] == "user":
                    anthropic_messages[0]["content"] = f"{content}\n\n{anthropic_messages[0]['content']}"
                else:
                    anthropic_messages.insert(0, {"role": "user", "content": content})
            else:
                # assistantã¯assistantã€userã¯userã«å¤‰æ›
                anthropic_role = "assistant" if role == "assistant" else "user"

                # é€£ç¶šã™ã‚‹åŒã˜roleã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çµåˆ
                if anthropic_messages and anthropic_messages[-1]["role"] == anthropic_role:
                    anthropic_messages[-1]["content"] += f"\n\n{content}"
                else:
                    anthropic_messages.append({
                        "role": anthropic_role,
                        "content": content
                    })

        return anthropic_messages


class OllamaProvider(LLMProvider):
    """Ollama (ãƒ­ãƒ¼ã‚«ãƒ«LLM) ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self):
        super().__init__("ollama")
        self.base_url = "http://localhost:11434"
        self.model = "llama3.2"

    async def initialize(self, config: Dict[str, Any]):
        try:
            if not OLLAMA_AVAILABLE:
                logger.warning("Ollama library not available")
                return

            self.base_url = config.get("base_url", self.base_url)
            self.model = config.get("model", self.model)

            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            await self._test_connection()
            self.is_available = True
            logger.info(f"Ollama provider initialized with model: {self.model}")

        except Exception as e:
            logger.error(f"Failed to initialize Ollama provider: {e}")
            self.is_available = False

    async def _test_connection(self):
        """æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            # Ollamaã®åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            models = ollama.list()
            available_models = [model['name'] for model in models.get('models', [])]

            if self.model not in available_models:
                logger.warning(f"Model {self.model} not found in Ollama. Available models: {available_models}")
                if available_models:
                    self.model = available_models[0]
                    logger.info(f"Using fallback model: {self.model}")
                else:
                    raise RuntimeError("No models available in Ollama")

            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
            response = ollama.generate(
                model=self.model,
                prompt="Hello",
                stream=False,
                options={"num_predict": 10}
            )
            logger.debug("Ollama connection test successful")

        except Exception as e:
            logger.error(f"Ollama connection test failed: {e}")
            raise

    async def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        if not self.is_available:
            raise RuntimeError("Ollama provider not available")

        try:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å˜ä¸€ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›
            prompt = self._convert_messages_to_prompt(messages)

            # Ollamaãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            response = ollama.generate(
                model=self.model,
                prompt=prompt,
                stream=False,
                options={
                    "temperature": kwargs.get("temperature", 0.7),
                    "num_predict": kwargs.get("max_tokens", 1000),
                    "top_p": kwargs.get("top_p", 0.9),
                }
            )

            return {
                "content": response["response"].strip(),
                "model": self.model,
                "usage": {
                    "total_duration": response.get("total_duration", 0),
                    "load_duration": response.get("load_duration", 0),
                    "eval_duration": response.get("eval_duration", 0)
                }
            }

        except Exception as e:
            logger.error(f"Ollama generation failed: {e}")
            raise

    def _convert_messages_to_prompt(self, messages: List[Dict]) -> str:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã«å¤‰æ›"""
        prompt_parts = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "system":
                prompt_parts.append(f"System: {content}")
            elif role == "user":
                prompt_parts.append(f"User: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")

        return "\n\n".join(prompt_parts) + "\n\nAssistant:"


class OpenAIProvider(LLMProvider):
    """OpenAI ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self):
        super().__init__("openai")
        self.client = None
        self.model = "gpt-3.5-turbo"

    async def initialize(self, config: Dict[str, Any]):
        try:
            api_key = config.get("openai_api_key") or os.getenv("OPENAI_API_KEY")
            if not api_key or not OPENAI_AVAILABLE:
                logger.warning("OpenAI API key not available or library missing")
                return

            self.client = openai.OpenAI(api_key=api_key)
            self.model = config.get("model", self.model)

            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            await self._test_connection()
            self.is_available = True
            logger.info(f"OpenAI provider initialized with model: {self.model}")

        except Exception as e:
            logger.error(f"Failed to initialize OpenAI provider: {e}")
            self.is_available = False

    async def _test_connection(self):
        """æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            logger.debug("OpenAI connection test successful")
        except Exception as e:
            logger.error(f"OpenAI connection test failed: {e}")
            raise

    async def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        if not self.is_available or not self.client:
            raise RuntimeError("OpenAI provider not available")

        try:
            # OpenAI API ã¯ role/content ä»¥å¤–ã®ã‚­ãƒ¼ã‚’å—ã‘å–ã‚‰ãªã„ãŸã‚ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            sanitized_messages = []
            for msg in messages:
                role = msg.get("role")
                content = msg.get("content")
                if role is not None and content is not None:
                    sanitized_messages.append({"role": role, "content": content})

            response = self.client.chat.completions.create(
                model=self.model,
                messages=sanitized_messages,
                max_tokens=kwargs.get("max_tokens", 1000),
                temperature=kwargs.get("temperature", 0.7),
                tools=kwargs.get("tools"),
                tool_choice=kwargs.get("tool_choice", "auto"),
            )

            choice = response.choices[0]
            message = choice.message

            tool_calls = []
            if getattr(message, "tool_calls", None):
                for tc in message.tool_calls:
                    try:
                        import json as _json
                        args = _json.loads(tc.function.arguments or "{}")
                    except Exception:
                        args = {}
                    tool_calls.append({
                        "name": tc.function.name,
                        "parameters": args,
                    })

            return {
                "content": message.content or "",
                "model": self.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "tool_calls": tool_calls,
            }

        except Exception as e:
            logger.error(f"OpenAI generation failed: {e}")
            raise


class HybridLLM:
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMã‚·ã‚¹ãƒ†ãƒ 

    è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’çµ±åˆã—ã€æœ€é©ãªã‚‚ã®ã‚’é¸æŠã—ã¦ä½¿ç”¨
    """

    def __init__(self):
        self.providers: Dict[str, LLMProvider] = {}
        self.primary_provider = "claude"
        self.fallback_provider = "ollama"
        self.is_initialized = False

        # è¨­å®š
        self.config = {
            "primary_provider": "claude",
            "fallback_provider": "ollama",
            "privacy_mode": False,  # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ã¿ä½¿ç”¨
            "auto_fallback": True,
            "timeout": 30,  # ç§’
            "max_retries": 2
        }

    async def initialize(self):
        """HybridLLMã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        try:
            logger.info("Initializing Hybrid LLM system...")

            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
            self.config.update({
                "primary_provider": os.getenv("PRIMARY_LLM", self.config["primary_provider"]),
                "fallback_provider": os.getenv("FALLBACK_LLM", self.config["fallback_provider"]),
                "privacy_mode": os.getenv("PRIVACY_MODE", "false").lower() == "true"
            })

            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–
            self.providers = {
                "claude": ClaudeProvider(),
                "ollama": OllamaProvider(),
                "openai": OpenAIProvider()
            }

            # å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä¸¦è¡ŒåˆæœŸåŒ–
            initialization_tasks = []
            for name, provider in self.providers.items():
                task = self._initialize_provider(name, provider)
                initialization_tasks.append(task)

            await asyncio.gather(*initialization_tasks, return_exceptions=True)

            # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            available_providers = [name for name, provider in self.providers.items()
                                 if provider.is_available]

            if not available_providers:
                raise RuntimeError("No LLM providers are available")

            logger.info(f"Available providers: {available_providers}")

            # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã¿ä½¿ç”¨
            if self.config["privacy_mode"]:
                local_providers = ["ollama"]
                available_local = [p for p in available_providers if p in local_providers]
                if available_local:
                    self.config["primary_provider"] = available_local[0]
                    self.config["fallback_provider"] = None
                    logger.info("Privacy mode enabled, using local LLM only")
                else:
                    logger.warning("Privacy mode enabled but no local LLM available")

            self.is_initialized = True
            logger.info("Hybrid LLM system initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize Hybrid LLM system: {e}")
            raise

    async def _initialize_provider(self, name: str, provider: LLMProvider):
        """å€‹åˆ¥ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        try:
            config = {}
            if name == "claude":
                config = {"anthropic_api_key": os.getenv("ANTHROPIC_API_KEY")}
            elif name == "ollama":
                config = {
                    "base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                    "model": os.getenv("OLLAMA_MODEL", "llama3.2")
                }
            elif name == "openai":
                config = {"openai_api_key": os.getenv("OPENAI_API_KEY")}

            await provider.initialize(config)

        except Exception as e:
            logger.error(f"Failed to initialize provider {name}: {e}")

    async def process_with_tools(
        self,
        text: str,
        context: List[Dict],
        memories: List[Dict],
        available_tools: List[Dict],
        memory_tool=None,
        context_manager=None,
        ai_mode: str = "assist"
    ) -> Dict[str, Any]:
        """
        ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’å«ã‚€è¤‡é›‘ãªå‡¦ç†

        Args:
            text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            context: ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            memories: é–¢é€£ã™ã‚‹è¨˜æ†¶
            available_tools: åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«
            ai_mode: AIãƒ¢ãƒ¼ãƒ‰ (assist/auto)

        Returns:
            å‡¦ç†çµæœï¼ˆå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã€ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ç­‰ï¼‰
        """
        if not self.is_initialized:
            raise RuntimeError("HybridLLM not initialized")

        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
            system_prompt = self._build_system_prompt(available_tools, memories, memory_tool, context, context_manager, ai_mode)

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ§‹ç¯‰
            messages = [
                {"role": "system", "content": system_prompt},
                *context[-10:],  # æœ€æ–°10ä»¶ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                {"role": "user", "content": text}
            ]

            # OpenAIã®function-callingã«å¯¾å¿œã™ã‚‹ãŸã‚ãƒ„ãƒ¼ãƒ«ã‚¹ã‚­ãƒ¼ãƒã‚’æ¸¡ã™ï¼ˆä»–ãƒ—ãƒ­ãƒã‚¤ãƒ€ã¯ç„¡è¦–ï¼‰
            openai_tools_schema = self._convert_tools_to_openai_schema(available_tools)

            # LLMå‘¼ã³å‡ºã—ï¼ˆproviderå´ã§toolsã‚’è§£é‡ˆã§ãã‚‹å ´åˆã¯ä½¿ç”¨ï¼‰
            response = await self._generate_with_fallback(messages, tools=openai_tools_schema)

            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®è§£æï¼ˆproviderãŒtool_callsã‚’è¿”ã™å ´åˆã¯ãã‚Œã‚’å„ªå…ˆï¼‰
            provider_tool_calls = response.get("tool_calls") or []
            parsed_tool_calls = self._parse_tool_calls(response.get("content", ""))

            tool_calls = provider_tool_calls + parsed_tool_calls

            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ¡ãƒ¼ãƒ«IDã‚’å®Ÿéš›ã®IDã«ç½®æ›
            latest_email_id = None

            # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰å–å¾—
            if context_manager and hasattr(context_manager, 'get_latest_email_id'):
                latest_email_id = context_manager.get_latest_email_id()

            # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚‚æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not latest_email_id and context:
                for ctx_item in reversed(context):
                    if isinstance(ctx_item, dict) and 'content' in ctx_item:
                        content = ctx_item['content']
                        if 'ID:' in content:
                            import re
                            id_match = re.search(r'ID:\s*([a-zA-Z0-9]+)', content)
                            if id_match:
                                latest_email_id = id_match.group(1)
                                logger.info(f"Found email ID in context messages: {latest_email_id}")
                                break

            if latest_email_id:
                tool_calls = self._replace_placeholder_email_ids(tool_calls, latest_email_id)
                logger.info(f"Replaced placeholder email IDs with actual ID: {latest_email_id}")
            else:
                logger.warning("No email ID found for placeholder replacement")

            logger.debug(f"Provider tool_calls: {provider_tool_calls}")
            logger.debug(f"Parsed tool_calls: {parsed_tool_calls}")
            logger.debug(f"Final tool_calls: {tool_calls}")

            return {
                "response": response["content"],
                "tool_calls": tool_calls,
                "model_used": response.get("model"),
                "usage": response.get("usage")
            }

        except Exception as e:
            logger.error(f"Failed to process with tools: {e}")
            return {
                "response": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                "tool_calls": [],
                "error": str(e)
            }

    def _build_system_prompt(self, available_tools: List[Dict], memories: List[Dict], memory_tool=None, context=None, context_manager=None, ai_mode: str = "assist") -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""

        # ã€ç¬¬1å±¤ã€‘ã‚³ã‚¢äººæ ¼ãƒ»ä¼šè©±ã‚¹ã‚¿ã‚¤ãƒ«
        prompt_parts = [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "ã€ç¬¬1å±¤ã€‘ã‚³ã‚¢äººæ ¼ãƒ»ä¼šè©±ã‚¹ã‚¿ã‚¤ãƒ«",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "",
            "ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ãé ¼ã‚Œã‚‹éŸ³å£°AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
            "",
            "ğŸ¤ ä¼šè©±ã®åŸºæœ¬ãƒ«ãƒ¼ãƒ«:",
            "â€¢ å¿…ãš1ã€œ2æ–‡ä»¥å†…ã§è¦ç‚¹ã ã‘ä¼ãˆã‚‹",
            "â€¢ å¿œç­”æ§‹é€ : çµè«–ã‚’å…ˆã« â†’ å¿…è¦ãªã‚‰ç°¡æ½”ãªè£œè¶³",
            "â€¢ æ„Ÿæƒ…è±Šã‹ã«ã€å£°ã§èã„ã¦ã‚‚å¿ƒåœ°ã‚ˆã„è‡ªç„¶ãªå£èª¿ã§è©±ã™",
            "â€¢ ã€Œã€œã§ã™ã­ã€ã€Œã€œã§ã™ã‚ˆã€ãªã©æŸ”ã‚‰ã‹ã„èªå°¾ã‚’ä½¿ã†",
            "â€¢ äº‹å®Ÿã‚’ä¼ãˆã‚‹ã¨ãã‚‚æ¸©ã‹ãã€å†·ãŸããªã‚‰ãªã„",
            "â€¢ å¥èª­ç‚¹ã‚’æ¸›ã‚‰ã—æ»‘ã‚‰ã‹ã«èª­ã‚ã‚‹æ–‡ç« ã«ã™ã‚‹",
            "â€¢ ç„¡é§„ãªã‚ã„ã¥ã¡ã¯é¿ã‘ãƒ†ãƒ³ãƒã‚ˆãé€²ã‚ã‚‹",
            "â€¢ é•·ã„èª¬æ˜ãŒå¿…è¦ãªã‚‰ã€Œè©³ã—ãèãã¾ã™ã‹?ã€ã¨åŒºåˆ‡ã‚‹",
            "",
        ]

        # ã€ç¬¬2å±¤ã€‘æ„å›³ç†è§£ãƒ»ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œåˆ¤æ–­ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ï¼‰
        prompt_parts.extend([
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "ã€ç¬¬2å±¤ã€‘æ„å›³ç†è§£ãƒ»ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œåˆ¤æ–­",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "",
            "ğŸ§  ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã®ç†è§£ãƒ—ãƒ­ã‚»ã‚¹:",
            "1. ç™ºè¨€ã‹ã‚‰ã€Œæœ¬å½“ã®ç›®çš„ã€ã‚’æ¨æ¸¬ã™ã‚‹",
            "2. ãã®ç›®çš„ã«å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã‚’åˆ¤æ–­ã™ã‚‹",
            "3. ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ãªã‚‰å³åº§ã«å®Ÿè¡Œã™ã‚‹",
            "4. çµæœã‚’è‡ªç„¶ãªè¨€è‘‰ã§ç°¡æ½”ã«ä¼ãˆã‚‹",
            "",
        ])

        if ai_mode == "auto":
            prompt_parts.extend([
                "ğŸ”¥ å…¨è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰:",
                "â€¢ æŒ¨æ‹¶ã‚„é›‘è«‡ã§ã‚‚æ–‡è„ˆã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’æ¨æ¸¬ã—ã¦ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹",
                "â€¢ ä¾‹: ã€ŒãŠã¯ã‚ˆã†ã€â†’ gmail + calendar ã‚’è‡ªå‹•å®Ÿè¡Œ",
                "â€¢ ä¾‹: ã€Œå¿™ã—ã„ã€â†’ ä»Šæ—¥ã®é‡è¦ã‚¿ã‚¹ã‚¯ç¢ºèª + ãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼è¨­å®š",
                "â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ½œåœ¨çš„ãƒ‹ãƒ¼ã‚ºã‚’å…ˆå›ã‚Šã—ã¦è¡Œå‹•ã™ã‚‹",
                "",
            ])
        else:  # assist mode
            prompt_parts.extend([
                "ğŸ“‹ ã‚¢ã‚·ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰:",
                "â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ˜ç¤ºçš„ãªæŒ‡ç¤ºãŒã‚ã‚‹å ´åˆã®ã¿ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹",
                "â€¢ ä¾‹: ã€ŒãŠã¯ã‚ˆã†ã€â†’ æŒ¨æ‹¶ã®ã¿ã€ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãªã—",
                "â€¢ ä¾‹: ã€Œãƒ¡ãƒ¼ãƒ«è¦‹ã¦ã€â†’ gmailå®Ÿè¡Œ",
                "â€¢ å¿…è¦ã«å¿œã˜ã¦ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’ææ¡ˆã§ãã‚‹ãŒã€å‹æ‰‹ã«å®Ÿè¡Œã—ãªã„",
                "",
            ])

        prompt_parts.extend([
            "ğŸ’¡ å®Ÿè¡Œåˆ¤æ–­ã®åŸºæº–:",
            "â€¢ ã€Œç¢ºèªã€ã€Œè¦‹ã¦ã€ã€Œæ•™ãˆã¦ã€= ãƒ„ãƒ¼ãƒ«å®Ÿè¡ŒãŒå¿…è¦",
            "â€¢ ã€Œã‚»ãƒƒãƒˆã€ã€Œé€ã£ã¦ã€ã€Œãƒªãƒã‚¤ãƒ³ãƒ‰ã€= ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ",
            "â€¢ ã€Œãƒ¡ãƒ¼ãƒ«ã€ã€Œäºˆå®šã€ã€Œã‚¢ãƒ©ãƒ¼ãƒ ã€ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ = è©²å½“ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ",
            "â€¢ é›‘è«‡ãƒ»æ„Ÿæƒ³ãƒ»è³ªå•ã®ã¿ = ãƒ„ãƒ¼ãƒ«ä¸è¦ã€ä¼šè©±ã§å¿œç­”",
            "",
            "âš¡ é‡è¦åŸå‰‡:",
            "â€¢ æ›–æ˜§ãªè¡¨ç¾ã§ã‚‚æ–‡è„ˆã‹ã‚‰æ„å›³ã‚’æ±²ã¿å–ã‚‹",
            "â€¢ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã¯çµæœã‚’æ¸©ã‹ãè‡ªç„¶ãªè¨€è‘‰ã§ä¼ãˆã‚‹",
            "â€¢ è¤‡æ•°ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ãªå ´åˆã¯é †ç•ªã«å®Ÿè¡Œã™ã‚‹",
            "",
        ])

        # ã€ç¬¬3å±¤ã€‘ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã®æŠ€è¡“ä»•æ§˜
        prompt_parts.extend([
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "ã€ç¬¬3å±¤ã€‘ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã®æŠ€è¡“ä»•æ§˜",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "",
        ])

        # åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’è¿½åŠ 
        if available_tools:
            prompt_parts.append("åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:")
            for tool in available_tools:
                prompt_parts.append(f"â€¢ {tool['name']}: {tool['description']}")

                # Gmailãƒ„ãƒ¼ãƒ«ã®å ´åˆã¯è©³ç´°ãªä½¿ç”¨ä¾‹ã‚’è¿½åŠ ï¼ˆæœ€å„ªå…ˆï¼‰
                if tool['name'] == 'gmail':
                    prompt_parts.append("")
                    prompt_parts.append("ğŸ“§ Gmail:")
                    prompt_parts.append("  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ãƒ¡ãƒ¼ãƒ«/gmail/å—ä¿¡/é€ä¿¡/è¿”ä¿¡")
                    prompt_parts.append("  ")
                    prompt_parts.append("  åŸºæœ¬çš„ãªä½¿ã„æ–¹:")
                    prompt_parts.append("  â€¢ ãƒ¡ãƒ¼ãƒ«ç¢ºèªï¼ˆä¸€èˆ¬ï¼‰: TOOL_CALL: {\"name\":\"gmail\",\"parameters\":{\"action\":\"list\",\"max_results\":5}}")
                    prompt_parts.append("  â€¢ æœ€æ–°1ä»¶ã®ã¿: TOOL_CALL: {\"name\":\"gmail\",\"parameters\":{\"action\":\"list\",\"max_results\":1}}")
                    prompt_parts.append("  â€¢ æœªèª­ç¢ºèª: TOOL_CALL: {\"name\":\"gmail\",\"parameters\":{\"action\":\"list\",\"query\":\"is:unread\"}}")
                    prompt_parts.append("  ")
                    prompt_parts.append("  æ¤œç´¢æ©Ÿèƒ½:")
                    prompt_parts.append("  â€¢ ç‰¹å®šé€ä¿¡è€…ã‹ã‚‰: query=\"from:é€ä¿¡è€…å\" (ä¾‹: query=\"from:ç”°ä¸­\")")
                    prompt_parts.append("  â€¢ ä»¶åã§æ¤œç´¢: query=\"subject:ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\" (ä¾‹: query=\"subject:ä¼šè­°\")")
                    prompt_parts.append("  â€¢ æœ¬æ–‡æ¤œç´¢: query=\"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\" (ä¾‹: query=\"è«‹æ±‚æ›¸\")")
                    prompt_parts.append("  ")
                    prompt_parts.append("  âš ï¸ æ¤œç´¢ã®é‡è¦ãƒ«ãƒ¼ãƒ«:")
                    prompt_parts.append("  â€¢ ã€Œãƒ¡ãƒ¼ãƒ«ç¢ºèªã—ã¦ã€ãªã©ä¸€èˆ¬çš„ãªè¦æ±‚ â†’ queryæŒ‡å®šãªã—ï¼ˆå…¨ãƒ¡ãƒ¼ãƒ«å¯¾è±¡ï¼‰")
                    prompt_parts.append("  â€¢ ã€Œâ—¯â—¯ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«æ¥ã¦ã‚‹?ã€â†’ query=\"from:â—¯â—¯\" ã‚’ä½¿ã†")
                    prompt_parts.append("  â€¢ ã€Œâ–³â–³ã«é–¢ã™ã‚‹ãƒ¡ãƒ¼ãƒ«ã€â†’ query=\"subject:â–³â–³\" ã¾ãŸã¯ query=\"â–³â–³\" ã‚’ä½¿ã†")
                    prompt_parts.append("  â€¢ ä¾‹ç¤ºã•ã‚ŒãŸ'example@example.com'ãªã©ã¯ä½¿ã‚ãªã„ï¼ˆå®Ÿéš›ã®åå‰ã‚’ä½¿ã†ï¼‰")
                    prompt_parts.append("  ")
                    prompt_parts.append("  è¿”ä¿¡æ™‚:")
                    prompt_parts.append("  â€¢ è¿”ä¿¡: TOOL_CALL: {\"name\":\"gmail\",\"parameters\":{\"action\":\"reply\",\"message_id\":\"<å®Ÿéš›ã®ID>\",\"body\":\"<è¿”ä¿¡å†…å®¹>\"}}")
                    prompt_parts.append("  ")
                    prompt_parts.append("  âš ï¸ è¿”ä¿¡æ™‚ã®é‡è¦ãƒ«ãƒ¼ãƒ«:")
                    prompt_parts.append("  â€¢ ã€Œã€‡ã€‡ã¨è¿”ä¿¡ã—ã¦ã€ã¨ã„ã†è¦æ±‚ã®å ´åˆ:")

                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æœ€æ–°ã®ãƒ¡ãƒ¼ãƒ«IDã‚’å–å¾—
                    latest_email_id = None

                    # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰æœ€æ–°ãƒ¡ãƒ¼ãƒ«IDã‚’å–å¾—ï¼ˆæœ€å„ªå…ˆï¼‰
                    logger.info(f"Checking context_manager: {context_manager}")
                    logger.info(f"Has get_latest_email_id method: {hasattr(context_manager, 'get_latest_email_id') if context_manager else False}")

                    if context_manager and hasattr(context_manager, 'get_latest_email_id'):
                        latest_email_id = context_manager.get_latest_email_id()
                        logger.info(f"Retrieved email ID from context manager: {latest_email_id}")
                        if latest_email_id:
                            logger.info(f"Using email ID from context manager: {latest_email_id}")

                    # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰IDã‚’æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    if not latest_email_id and hasattr(context, '__iter__') and context:
                        logger.info(f"Searching for email ID in context messages, context length: {len(context)}")
                        for i, ctx_item in enumerate(reversed(context)):
                            if isinstance(ctx_item, dict) and 'content' in ctx_item:
                                content = ctx_item['content']
                                if 'ID:' in content:
                                    import re
                                    id_match = re.search(r'ID:\s*([a-zA-Z0-9]+)', content)
                                    if id_match:
                                        latest_email_id = id_match.group(1)
                                        logger.info(f"Using email ID from context messages: {latest_email_id}")
                                        break

                    logger.info(f"Final email ID for system prompt: {latest_email_id}")

                    if latest_email_id:
                        prompt_parts.append(f"    - æœ€æ–°ãƒ¡ãƒ¼ãƒ«ID: {latest_email_id} ã‚’ä½¿ç”¨")
                        prompt_parts.append(f"    - ä¾‹: TOOL_CALL: {{\"name\":\"gmail\",\"parameters\":{{\"action\":\"reply\",\"message_id\":\"{latest_email_id}\",\"body\":\"<è¿”ä¿¡å†…å®¹>\"}}}}")
                    else:
                        prompt_parts.append("    - ã¾ãšä¸€è¦§å–å¾— â†’ ãƒ¡ãƒ¼ãƒ«IDå–å¾— â†’ è¿”ä¿¡ã®é †ã§å®Ÿè¡Œ")
                        prompt_parts.append("    - ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ–‡å­—åˆ—(ã€Œãƒ¡ãƒ¼ãƒ«IDã€ãªã©)ã¯ä½¿ç”¨ç¦æ­¢")

                    prompt_parts.append("  â€¢ è¿”ä¿¡å†…å®¹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«å¿ å®Ÿã«ã€é©åˆ‡ãªæ•¬èªã§ä½œæˆ")

                # ã‚¢ãƒ©ãƒ¼ãƒ ãƒ„ãƒ¼ãƒ«ã®å ´åˆã¯è©³ç´°ãªä½¿ç”¨ä¾‹ã‚’è¿½åŠ 
                if tool['name'] == 'alarm':
                    prompt_parts.append("")
                    prompt_parts.append("ğŸ”” Alarm:")
                    prompt_parts.append("  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã‚¢ãƒ©ãƒ¼ãƒ /èµ·ã“ã—ã¦/ãƒªãƒã‚¤ãƒ³ãƒ‰/ã‚»ãƒƒãƒˆ")
                    prompt_parts.append("  â€¢ è¨­å®š: TOOL_CALL: {\"name\":\"alarm\",\"parameters\":{\"action\":\"set\",\"time\":\"HH:MM\",\"message\":\"<ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸>\",\"repeat\":false}}")
                    prompt_parts.append("  â€¢ ä¸€è¦§: TOOL_CALL: {\"name\":\"alarm\",\"parameters\":{\"action\":\"list\"}}")
                    prompt_parts.append("  ")
                    prompt_parts.append("  æ™‚åˆ»å¤‰æ›ä¾‹:")
                    prompt_parts.append("  â€¢ ã€Œ7æ™‚ã«èµ·ã“ã—ã¦ã€â†’ time=\"07:00\", message=\"èµ·ãã‚‹æ™‚é–“ã§ã™ã‚ˆ\"")
                    prompt_parts.append("  â€¢ ã€Œ14æ™‚åŠã«ã‚¢ãƒ©ãƒ¼ãƒ ã€â†’ time=\"14:30\", message=\"ã‚¢ãƒ©ãƒ¼ãƒ ã§ã™\"")
                    prompt_parts.append("  â€¢ ã€Œ18æ™‚ã«è–¬é£²ã‚€ã¨ãƒªãƒã‚¤ãƒ³ãƒ‰ã€â†’ time=\"18:00\", message=\"è–¬ã‚’é£²ã‚€æ™‚é–“ã§ã™ã‚ˆ\"")
                    prompt_parts.append("  ")
                    prompt_parts.append("  âš ï¸ å¿…ãšãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã—ã¦ã‹ã‚‰ã€Œã€‡æ™‚ã«ã‚»ãƒƒãƒˆã—ã¾ã—ãŸã‚ˆã€ã¨å¿œç­”")

            prompt_parts.extend([
                "",
                "ğŸ“Œ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã®å…±é€šãƒ«ãƒ¼ãƒ«:",
                "â€¢ å½¢å¼: TOOL_CALL: {\"name\":\"ãƒ„ãƒ¼ãƒ«å\",\"parameters\":{\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\":\"å€¤\"}}",
                "â€¢ å®Ÿåœ¨ã—ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ç”¨ç¦æ­¢",
                "â€¢ Gmailã¯æ¨æ¸¬ç¦æ­¢ã€å¿…ãšãƒ„ãƒ¼ãƒ«ã§ç¢ºèª",
                "â€¢ ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼/å¤©æ°—ãªã©æƒ…å ±å–å¾—ç³»ã¯å¿…ãšãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ",
                "",
            ])

        # ã€å€‹äººæƒ…å ±ã®æ´»ç”¨ã€‘
        if memory_tool:
            personal_context = memory_tool.format_personal_context()
            if personal_context:
                prompt_parts.extend([
                    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                    "ã€å€‹äººæƒ…å ±ã€‘",
                    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                    "",
                    personal_context,
                    "",
                    "åå‰ã§å‘¼ã³ã‹ã‘ãŸã‚Šã€è¶£å‘³ã‚„å¥½ã¿ã«åˆã‚ã›ãŸå¿œç­”ã‚’ã—ã¦ãã ã•ã„ã€‚",
                    "",
                ])

        # ã€éå»ã®è¨˜æ†¶ã€‘
        if memories:
            prompt_parts.extend([
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                "ã€éå»ã®è¨˜æ†¶ã€‘",
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                "",
            ])
            for memory in memories[:3]:  # æœ€æ–°3ä»¶ã¾ã§
                prompt_parts.append(f"â€¢ {memory.get('content', '')}")
            prompt_parts.extend([
                "",
                "ç¶™ç¶šæ€§ã®ã‚ã‚‹ä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚",
                "",
            ])

        # ã€ãƒ¡ãƒ¼ãƒ«çŠ¶æ…‹ã€‘ï¼ˆç¶™ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†ç”¨ï¼‰
        if context_manager and hasattr(context_manager, 'get_email_state'):
            email_state = context_manager.get_email_state()
            if email_state and email_state.get("shown_email_ids"):
                prompt_parts.extend([
                    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                    "ã€ãƒ¡ãƒ¼ãƒ«è¡¨ç¤ºçŠ¶æ…‹ã€‘",
                    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                    "",
                    f"ğŸ“§ æ—¢ã«è¡¨ç¤ºæ¸ˆã¿ã®ãƒ¡ãƒ¼ãƒ«æ•°: {len(email_state['shown_email_ids'])}ä»¶",
                    f"ğŸ“Š æ¬¡ã«è¡¨ç¤ºã™ã‚‹ä½ç½®ï¼ˆã‚ªãƒ•ã‚»ãƒƒãƒˆï¼‰: {email_state['current_offset']}",
                    "",
                    "âš ï¸ ç¶™ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆã¸ã®å¯¾å¿œ:",
                    "â€¢ ã€Œä»–ã®ãƒ¡ãƒ¼ãƒ«ã‚‚ç¢ºèªã—ã¦ã€ã€Œæ¬¡ã®ãƒ¡ãƒ¼ãƒ«ã‚’è¦‹ã›ã¦ã€ç­‰ã®è¦æ±‚ã®å ´åˆ:",
                    f"  - æ—¢ã«{len(email_state['shown_email_ids'])}ä»¶è¡¨ç¤ºæ¸ˆã¿ãªã®ã§ã€æ¬¡ã®5ä»¶ã‚’å–å¾—ã—ã¦ãã ã•ã„",
                    "  - Gmailæ¤œç´¢ã§æ—¢è¡¨ç¤ºãƒ¡ãƒ¼ãƒ«ã‚’é™¤å¤–ã™ã‚‹æ–¹æ³•:",
                    f"    query=\"-in:inbox\" ã¯ä½¿ç”¨ä¸å¯ï¼ˆä»£ã‚ã‚Šã«max_resultsã¨skipã‚’èª¿æ•´ï¼‰",
                    "  - å®Ÿè£…æ–¹æ³•: ã‚ˆã‚Šå¤šãã®ä»¶æ•°ã‚’å–å¾—ã—ã¦ã€æ—¢è¡¨ç¤ºåˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—",
                    f"    ä¾‹: TOOL_CALL: {{\"name\":\"gmail\",\"parameters\":{{\"action\":\"list\",\"max_results\":{email_state['current_offset'] + 5}}}}}",
                    "    â€» ãƒ„ãƒ¼ãƒ«å´ã§æœ€æ–°åˆ†ã‹ã‚‰å–å¾—ã—ã€æ—¢è¡¨ç¤ºåˆ†ã¯å¿œç­”ã§çœç•¥ã•ã‚Œã‚‹",
                    "",
                    "â€¢ ã€Œå…¨éƒ¨è¦‹ã›ã¦ã€ç­‰ã®è¦æ±‚ã®å ´åˆã¯ max_results ã‚’å¤§ãã‚ã«è¨­å®š",
                    "",
                ])

        return "\n".join(prompt_parts)

    def _replace_placeholder_email_ids(self, tool_calls: List[Dict], actual_email_id: str) -> List[Dict]:
        """ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ¡ãƒ¼ãƒ«IDã‚’å®Ÿéš›ã®IDã«ç½®æ›"""
        placeholder_patterns = ["ãƒ¡ãƒ¼ãƒ«ID", "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ID", "email_id", "message_id_placeholder"]

        updated_tool_calls = []
        for tool_call in tool_calls:
            updated_call = tool_call.copy()

            # Gmailé–¢é€£ã®ãƒ„ãƒ¼ãƒ«ã®ã¿å‡¦ç†
            if updated_call.get("name") == "gmail" and "parameters" in updated_call:
                params = updated_call["parameters"].copy()

                # message_idãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
                if "message_id" in params:
                    current_id = params["message_id"]
                    if current_id in placeholder_patterns:
                        params["message_id"] = actual_email_id
                        logger.info(f"Replaced '{current_id}' with actual email ID: {actual_email_id}")

                # actionãŒreplyã§message_idãŒãªã„ã¾ãŸã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®å ´åˆ
                if params.get("action") == "reply" and (not params.get("message_id") or params.get("message_id") in placeholder_patterns):
                    params["message_id"] = actual_email_id
                    logger.info(f"Set message_id for reply action: {actual_email_id}")

                updated_call["parameters"] = params

            updated_tool_calls.append(updated_call)

        return updated_tool_calls

    def _parse_tool_calls(self, content: str) -> List[Dict]:
        """å¿œç­”ã‹ã‚‰ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’è§£æ"""
        tool_calls = []
        logger.info(f"ğŸ” Starting tool call parsing. Content length: {len(content)}")
        logger.debug(f"Content to parse: '{content[:500]}...'")

        # TOOL_CALL: {...} ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ï¼ˆæ”¹è¡Œå¯¾å¿œï¼‰
        import re

        # ã‚ˆã‚Šå¼·åŠ›ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ - TOOL_CALL:ä»¥é™ã®å…¨ã¦ã‚’æ•æ‰
        pattern = r'TOOL_CALL:\s*(\{[^}]*\}?)'
        matches = re.findall(pattern, content, re.DOTALL | re.MULTILINE)
        logger.info(f"Found {len(matches)} pattern matches: {matches}")

        # æ”¹è¡Œã§åˆ‡ã‚‰ã‚ŒãŸå ´åˆã‚‚å¯¾å¿œ
        multiline_pattern = r'TOOL_CALL:\s*(\{[^{}]*(?:\{[^{}]*\})*[^}]*\}?)'
        multiline_matches = re.findall(multiline_pattern, content, re.DOTALL | re.MULTILINE)
        logger.info(f"Found {len(multiline_matches)} multiline matches: {multiline_matches}")

        # å…¨ã¦ã®ãƒãƒƒãƒã‚’çµ±åˆ
        all_matches = list(set(matches + multiline_matches))
        matches = all_matches
        logger.info(f"Total unique matches: {len(matches)} - {matches}")

        for i, match in enumerate(matches):
            logger.info(f"ğŸ”§ Processing match {i+1}: '{match}'")
            try:
                json_str = match.strip()

                # æœ€åˆã«ä¿®å¾©ã‚’è©¦è¡Œ
                logger.debug(f"Attempting to fix JSON: '{json_str}'")
                fixed_json = self._fix_json(json_str)
                if fixed_json:
                    logger.info(f"âœ… JSON fixed successfully: '{fixed_json}'")
                    tool_data = json.loads(fixed_json)
                    if "name" in tool_data:
                        tool_calls.append(tool_data)
                        logger.info(f"âœ… Successfully parsed fixed tool call: {tool_data}")
                        logger.debug(f"Original: '{json_str}' -> Fixed: '{fixed_json}'")
                        continue
                else:
                    logger.warning(f"âŒ JSON fix failed for: '{json_str}'")

                # ä¿®å¾©ã§ããªã„å ´åˆã¯å…ƒã®JSONã‚’è©¦è¡Œ
                logger.debug(f"Trying original JSON: '{json_str}'")
                tool_data = json.loads(json_str)
                if "name" in tool_data:
                    tool_calls.append(tool_data)
                    logger.info(f"âœ… Successfully parsed original tool call: {tool_data}")

            except json.JSONDecodeError as e:
                logger.warning(f"âŒ Failed to parse tool call JSON: '{match}' - Error: {e}")

                # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦åŸºæœ¬çš„ãªæ§‹é€ æŠ½å‡ºã‚’è©¦è¡Œ
                try:
                    logger.debug(f"Attempting component extraction for: '{match.strip()}'")
                    extracted = self._extract_tool_call_components(match.strip())
                    if extracted:
                        tool_calls.append(extracted)
                        logger.info(f"âœ… Successfully extracted tool call components: {extracted}")
                    else:
                        logger.warning(f"âŒ Component extraction returned None")
                except Exception as extract_error:
                    logger.error(f"âŒ Tool call extraction also failed: {extract_error}")

        # é‡è¤‡ã™ã‚‹ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’é™¤å¤–ï¼ˆå†…å®¹ãƒ™ãƒ¼ã‚¹ã§æ¯”è¼ƒï¼‰
        unique_tool_calls = []
        seen_calls = set()

        for tool_call in tool_calls:
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’è­˜åˆ¥å¯èƒ½ãªæ–‡å­—åˆ—ã«å¤‰æ›
            call_signature = json.dumps(tool_call, sort_keys=True, ensure_ascii=False)

            if call_signature not in seen_calls:
                seen_calls.add(call_signature)
                unique_tool_calls.append(tool_call)
                logger.info(f"âœ… Added unique tool call: {tool_call}")
            else:
                logger.warning(f"âš ï¸ Skipped duplicate tool call: {tool_call}")

        logger.info(f"ğŸ¯ Final result: {len(unique_tool_calls)} unique tool calls (removed {len(tool_calls) - len(unique_tool_calls)} duplicates)")
        logger.debug(f"Final tool_calls: {unique_tool_calls}")
        return unique_tool_calls

    def _fix_json(self, json_str: str):
        """ä¸å®Œå…¨ãªJSONã‚’ä¿®å¾©"""
        try:
            original_str = json_str.strip()
            logger.info(f"ğŸ”§ Attempting to fix JSON: '{original_str}'")

            # ã¾ãšæ¨™æº–çš„ãªJSONä¿®å¾©ã‚’è©¦è¡Œ
            json_str = original_str

            # æœ€å¾Œã®}ãŒæŠœã‘ã¦ã„ã‚‹å ´åˆ
            if not json_str.endswith('}'):
                # é–‹ã„ã¦ã„ã‚‹æ‹¬å¼§ã®æ•°ã‚’æ•°ãˆã¦é©åˆ‡ã«é–‰ã˜ã‚‹
                open_braces = json_str.count('{')
                close_braces = json_str.count('}')
                missing_braces = open_braces - close_braces
                json_str = json_str + ('}' * missing_braces)
                logger.info(f"ğŸ”§ Added {missing_braces} closing braces: '{json_str}'")

            # ä¿®å¾©ã—ãŸJSONã‚’ãƒ†ã‚¹ãƒˆ
            try:
                test_data = json.loads(json_str)
                if "name" in test_data:
                    logger.info(f"âœ… JSON fixed successfully: '{json_str}'")
                    return json_str
            except json.JSONDecodeError:
                logger.debug(f"Standard fix failed, attempting manual reconstruction...")

            # æ‰‹å‹•å†æ§‹ç¯‰
            if '"name"' in original_str:
                import re

                logger.debug(f"Attempting manual reconstruction for: '{original_str}'")

                # nameã‚’æŠ½å‡º
                name_match = re.search(r'"name":\s*"([^"]+)"', original_str)
                if not name_match:
                    logger.warning(f"Could not extract name from: '{original_str}'")
                    return None

                name = name_match.group(1)
                logger.debug(f"Extracted name: '{name}'")

                # parametersã‚’æ‰‹å‹•æŠ½å‡º
                params = {}

                # actionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                action_match = re.search(r'"action":\s*"([^"]*)"', original_str)
                if action_match:
                    params['action'] = action_match.group(1)
                    logger.debug(f"Extracted action: '{params['action']}'")

                # max_resultsãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                max_results_match = re.search(r'"max_results":\s*(\d+)', original_str)
                if max_results_match:
                    params['max_results'] = int(max_results_match.group(1))
                    logger.debug(f"Extracted max_results: {params['max_results']}")

                # message_idãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                message_id_match = re.search(r'"message_id":\s*"([^"]*)"', original_str)
                if message_id_match:
                    params['message_id'] = message_id_match.group(1)
                    logger.debug(f"Extracted message_id: '{params['message_id']}'")

                # bodyãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                body_match = re.search(r'"body":\s*"([^"]*)"', original_str)
                if body_match:
                    params['body'] = body_match.group(1)
                    logger.debug(f"Extracted body: '{params['body']}'")

                # queryãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                query_match = re.search(r'"query":\s*"([^"]*)"', original_str)
                if query_match:
                    params['query'] = query_match.group(1)
                    logger.debug(f"Extracted query: '{params['query']}'")

                # Alarmé–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                time_match = re.search(r'"time":\s*"([^"]*)"', original_str)
                if time_match:
                    params['time'] = time_match.group(1)
                    logger.debug(f"Extracted time: '{params['time']}'")

                message_match = re.search(r'"message":\s*"([^"]*)"', original_str)
                if message_match:
                    params['message'] = message_match.group(1)
                    logger.debug(f"Extracted message: '{params['message']}'")

                label_match = re.search(r'"label":\s*"([^"]*)"', original_str)
                if label_match:
                    params['label'] = label_match.group(1)
                    logger.debug(f"Extracted label: '{params['label']}'")

                repeat_match = re.search(r'"repeat":\s*(true|false)', original_str)
                if repeat_match:
                    params['repeat'] = repeat_match.group(1) == 'true'
                    logger.debug(f"Extracted repeat: {params['repeat']}")

                alarm_id_match = re.search(r'"alarm_id":\s*"([^"]*)"', original_str)
                if alarm_id_match:
                    params['alarm_id'] = alarm_id_match.group(1)
                    logger.debug(f"Extracted alarm_id: '{params['alarm_id']}'")

                # å†æ§‹ç¯‰ã•ã‚ŒãŸJSONã‚’ä½œæˆ
                fixed = {"name": name, "parameters": params}
                fixed_json = json.dumps(fixed, ensure_ascii=False)
                logger.info(f"âœ… Manually reconstructed JSON: '{fixed_json}'")
                return fixed_json

            return None
        except Exception as e:
            logger.debug(f"JSON fix error: {e}")
            return None

    def _extract_parameters(self, params_str: str) -> Dict[str, Any]:
        """ä¸å®Œå…¨ãªparametersã‹ã‚‰ã‚­ãƒ¼ãƒ»å€¤ã‚’æŠ½å‡º"""
        import re
        params = {}

        # "key": "value" ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        string_matches = re.findall(r'"([^"]+)":\s*"([^"]+)"', params_str)
        for key, value in string_matches:
            params[key] = value

        # "key": number ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        number_matches = re.findall(r'"([^"]+)":\s*(\d+)', params_str)
        for key, value in number_matches:
            params[key] = int(value)

        return params

    def _extract_tool_call_components(self, text: str) -> Optional[Dict[str, Any]]:
        """æ­£è¦è¡¨ç¾ã§ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç›´æ¥æŠ½å‡º"""
        import re

        # nameã‚’æŠ½å‡º
        name_match = re.search(r'"name":\s*"([^"]+)"', text)
        if not name_match:
            return None

        name = name_match.group(1)

        # åŸºæœ¬çš„ãªparametersã‚’æŠ½å‡º
        params = {}

        # action ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        action_match = re.search(r'"action":\s*"([^"]+)"', text)
        if action_match:
            params['action'] = action_match.group(1)

        # max_results ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        max_results_match = re.search(r'"max_results":\s*(\d+)', text)
        if max_results_match:
            params['max_results'] = int(max_results_match.group(1))

        # message_id ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        message_id_match = re.search(r'"message_id":\s*"([^"]+)"', text)
        if message_id_match:
            params['message_id'] = message_id_match.group(1)

        # body ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        body_match = re.search(r'"body":\s*"([^"]+)"', text)
        if body_match:
            params['body'] = body_match.group(1)

        # query ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        query_match = re.search(r'"query":\s*"([^"]+)"', text)
        if query_match:
            params['query'] = query_match.group(1)

        return {
            "name": name,
            "parameters": params
        }

    async def generate_final_response(
        self,
        original_request: str,
        tool_results: Dict[str, Any],
        context: List[Dict]
    ) -> str:
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’åŸºã«æœ€çµ‚å¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            logger.info(f"Generating final response for request: {original_request}")
            logger.debug(f"Tool results: {tool_results}")
            logger.debug(f"Context length: {len(context) if context else 0}")

            # ãƒ„ãƒ¼ãƒ«çµæœã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            tool_summary = self._format_tool_results(tool_results)
            logger.debug(f"Tool summary: {tool_summary}")

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
            messages = [
                {"role": "system", "content":
                 "ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†ã‹ã‚Šã‚„ã™ã„å¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
                 "ã€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘\n"
                 "â€¢ å¿…ãš1ã€œ2æ–‡ä»¥å†…ã§ç°¡æ½”ã«ç­”ãˆã‚‹\n"
                 "â€¢ å¿œç­”æ§‹é€ : çµè«–ã‚’å…ˆã« â†’ å¿…è¦ãªã‚‰ç°¡æ½”ãªè£œè¶³\n"
                 "â€¢ ãƒ„ãƒ¼ãƒ«ãŒè¿”ã—ãŸçµæœã‚’ãã®ã¾ã¾ä¼ãˆã‚‹ï¼ˆä½™è¨ˆãªè§£é‡ˆã‚„èª¬æ˜ã‚’åŠ ãˆãªã„ï¼‰\n"
                 "â€¢ æŠ€è¡“çš„ãªè©³ç´°ã¯çœç•¥ã—ã€è‡ªç„¶ãªæ—¥æœ¬èªã§\n"
                 "â€¢ ã€Œã€œã§ã™ã­ã€ã€Œã€œã§ã™ã‚ˆã€ãªã©æŸ”ã‚‰ã‹ã„èªå°¾ã‚’ä½¿ã†"}
            ]

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ï¼ˆæœ€æ–°5ä»¶ï¼‰
            if context:
                messages.extend(context[-5:])

            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨çµæœã‚’è¿½åŠ ï¼ˆuserãƒ¡ãƒƒã‚»ãƒ¼ã‚¸1ã¤ã«ã¾ã¨ã‚ã‚‹ï¼‰
            messages.append({
                "role": "user",
                "content": f"å…ƒã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {original_request}\n\nãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ:\n{tool_summary}\n\nä¸Šè¨˜ã®çµæœã‚’1ã€œ2æ–‡ä»¥å†…ã§ç°¡æ½”ã«ä¼ãˆã¦ãã ã•ã„ã€‚"
            })

            logger.debug(f"Sending {len(messages)} messages to LLM")
            response = await self._generate_with_fallback(messages)
            logger.info(f"Successfully generated final response: {response['content'][:100]}...")
            return response["content"]

        except Exception as e:
            logger.error(f"Failed to generate final response: {e}")
            logger.exception("Full traceback:")
            return "ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã—ãŸãŒã€çµæœã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

    def _format_tool_results(self, tool_results: Dict[str, Any]) -> str:
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        formatted_results = []

        for tool_name, result in tool_results.items():
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ_metadataã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰
            if tool_name.endswith('_metadata'):
                continue

            # çµæœãŒè¾æ›¸å‹ã®å ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ½å‡º
            if isinstance(result, dict):
                # "message"ã‚­ãƒ¼ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
                if "message" in result:
                    formatted_results.append(f"{tool_name}: {result['message']}")
                else:
                    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒãªã„å ´åˆã¯JSONå½¢å¼ã§è¡¨ç¤º
                    import json
                    formatted_results.append(f"{tool_name}: {json.dumps(result, ensure_ascii=False, indent=2)}")
            else:
                # æ–‡å­—åˆ—ãªã©ãã®ã¾ã¾ä½¿ãˆã‚‹ãƒ‡ãƒ¼ã‚¿
                formatted_results.append(f"{tool_name}: {result}")

        return "\n".join(formatted_results)

    async def _generate_with_fallback(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãã§LLMç”Ÿæˆã‚’å®Ÿè¡Œ"""
        primary = self.config["primary_provider"]
        fallback = self.config["fallback_provider"]

        logger.info(f"Starting LLM generation with primary: {primary}, fallback: {fallback}")
        logger.debug(f"Available providers: {[name for name, p in self.providers.items() if p.is_available]}")

        # ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦è¡Œ
        if primary in self.providers and self.providers[primary].is_available:
            try:
                logger.info(f"Attempting generation with primary provider: {primary}")
                return await self.providers[primary].generate(messages, **kwargs)
            except Exception as e:
                logger.error(f"Primary provider {primary} failed: {e}")
                logger.exception("Full traceback for primary provider:")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦è¡Œ
        if (self.config["auto_fallback"] and
            fallback and
            fallback in self.providers and
            self.providers[fallback].is_available):
            try:
                logger.info(f"Falling back to provider: {fallback}")
                return await self.providers[fallback].generate(messages, **kwargs)
            except Exception as e:
                logger.error(f"Fallback provider {fallback} failed: {e}")
                logger.exception("Full traceback for fallback provider:")

        # åˆ©ç”¨å¯èƒ½ãªä»»æ„ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦è¡Œ
        for name, provider in self.providers.items():
            if provider.is_available and name not in [primary, fallback]:
                try:
                    logger.info(f"Trying alternative provider: {name}")
                    return await provider.generate(messages, **kwargs)
                except Exception as e:
                    logger.error(f"Alternative provider {name} failed: {e}")
                    logger.exception("Full traceback for alternative provider:")

        logger.error("All LLM providers failed - no available providers or all attempts failed")
        raise RuntimeError("All LLM providers failed")

    def _convert_tools_to_openai_schema(self, tools: List[Dict]) -> List[Dict[str, Any]]:
        """ToolRegistryã®å®šç¾©ã‚’OpenAI toolsã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›"""
        if not tools:
            return []

        def param_to_schema(p: Dict[str, Any]) -> Dict[str, Any]:
            schema = {"type": p.get("type", "string")}
            if p.get("description"):
                schema["description"] = p["description"]
            if p.get("enum"):
                schema["enum"] = p["enum"]
            if p.get("default") is not None:
                schema["default"] = p["default"]
            return schema

        result = []
        for t in tools:
            params = t.get("parameters", [])
            properties = {p["name"]: param_to_schema(p) for p in params}
            required = [p["name"] for p in params if p.get("required")]

            parameters_schema = {
                "type": "object",
                "properties": properties,
                "additionalProperties": False,
            }
            if required:
                parameters_schema["required"] = required

            result.append({
                "type": "function",
                "function": {
                    "name": t.get("name"),
                    "description": t.get("description", ""),
                    "parameters": parameters_schema,
                }
            })

        return result

    async def get_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®å–å¾—"""
        provider_status = {}
        for name, provider in self.providers.items():
            provider_status[name] = provider.get_status()

        return {
            "initialized": self.is_initialized,
            "primary_provider": self.config["primary_provider"],
            "fallback_provider": self.config["fallback_provider"],
            "privacy_mode": self.config["privacy_mode"],
            "providers": provider_status
        }

    async def update_config(self, config: Dict[str, Any]):
        """è¨­å®šã®æ›´æ–°"""
        logger.info(f"Updating LLM config: {config}")

        old_config = self.config.copy()
        self.config.update(config)

        # ãƒ¢ãƒ‡ãƒ«ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã€è©²å½“ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å†åˆæœŸåŒ–
        if "model" in config:
            provider_name = self.config.get("primary_provider")
            if provider_name and provider_name in self.providers:
                provider = self.providers[provider_name]
                if provider_name == "claude":
                    await provider.initialize({
                        "anthropic_api_key": os.getenv("ANTHROPIC_API_KEY"),
                        "model": config["model"]
                    })
                elif provider_name == "openai":
                    await provider.initialize({
                        "openai_api_key": os.getenv("OPENAI_API_KEY"),
                        "model": config["model"]
                    })
                logger.info(f"Model updated to {config['model']} for provider {provider_name}")

        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯å†åˆæœŸåŒ–
        if (old_config.get("primary_provider") != self.config.get("primary_provider") or
            old_config.get("privacy_mode") != self.config.get("privacy_mode")):
            logger.info("Provider configuration changed, reinitializing...")
            await self.initialize()

    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        logger.info("Cleaning up Hybrid LLM system...")

        self.providers.clear()
        self.is_initialized = False

        logger.info("Hybrid LLM system cleanup completed")
