"""
Hybrid LLM - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMã‚·ã‚¹ãƒ†ãƒ 

Claudeã¨Ollamaã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã¨æ€§èƒ½ã‚’ä¸¡ç«‹ã™ã‚‹LLMã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import json
import asyncio
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
from loguru import logger

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("Anthropic library not available")

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("Ollama library not available")

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI library not available")


class LLMProvider:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, name: str):
        self.name = name
        self.is_available = False

    async def initialize(self, config: Dict[str, Any]):
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        pass

    async def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        raise NotImplementedError

    async def is_healthy(self) -> bool:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        return self.is_available

    def get_status(self) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®çŠ¶æ…‹å–å¾—"""
        return {
            "name": self.name,
            "available": self.is_available
        }


class ClaudeProvider(LLMProvider):
    """Claude (Anthropic) ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self):
        super().__init__("claude")
        self.client = None
        self.model = "claude-3-haiku-20240307"

    async def initialize(self, config: Dict[str, Any]):
        try:
            api_key = config.get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")
            if not api_key or not ANTHROPIC_AVAILABLE:
                logger.warning("Claude API key not available or library missing")
                return

            self.client = anthropic.Anthropic(api_key=api_key)
            self.model = config.get("model", self.model)

            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            await self._test_connection()
            self.is_available = True
            logger.info(f"Claude provider initialized with model: {self.model}")

        except Exception as e:
            logger.error(f"Failed to initialize Claude provider: {e}")
            self.is_available = False

    async def _test_connection(self):
        """æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            response = self.client.messages.create(
                model=self.model,
                max_tokens=10,
                messages=[{"role": "user", "content": "Hello"}]
            )
            logger.debug("Claude connection test successful")
        except Exception as e:
            logger.error(f"Claude connection test failed: {e}")
            raise

    async def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        if not self.is_available or not self.client:
            raise RuntimeError("Claude provider not available")

        try:
            # Anthropicãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
            anthropic_messages = self._convert_messages(messages)

            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            response = self.client.messages.create(
                model=self.model,
                max_tokens=kwargs.get("max_tokens", 1000),
                temperature=kwargs.get("temperature", 0.7),
                messages=anthropic_messages
            )

            return {
                "content": response.content[0].text,
                "model": self.model,
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens
                }
            }

        except Exception as e:
            logger.error(f"Claude generation failed: {e}")
            raise

    def _convert_messages(self, messages: List[Dict]) -> List[Dict]:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’Anthropicå½¢å¼ã«å¤‰æ›"""
        anthropic_messages = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯æœ€åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«çµåˆ
            if role == "system":
                if anthropic_messages and anthropic_messages[0]["role"] == "user":
                    anthropic_messages[0]["content"] = f"{content}\n\n{anthropic_messages[0]['content']}"
                else:
                    anthropic_messages.insert(0, {"role": "user", "content": content})
            else:
                # assistantã¯assistantã€userã¯userã«å¤‰æ›
                anthropic_role = "assistant" if role == "assistant" else "user"
                anthropic_messages.append({
                    "role": anthropic_role,
                    "content": content
                })

        return anthropic_messages


class OllamaProvider(LLMProvider):
    """Ollama (ãƒ­ãƒ¼ã‚«ãƒ«LLM) ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self):
        super().__init__("ollama")
        self.base_url = "http://localhost:11434"
        self.model = "llama3.2"

    async def initialize(self, config: Dict[str, Any]):
        try:
            if not OLLAMA_AVAILABLE:
                logger.warning("Ollama library not available")
                return

            self.base_url = config.get("base_url", self.base_url)
            self.model = config.get("model", self.model)

            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            await self._test_connection()
            self.is_available = True
            logger.info(f"Ollama provider initialized with model: {self.model}")

        except Exception as e:
            logger.error(f"Failed to initialize Ollama provider: {e}")
            self.is_available = False

    async def _test_connection(self):
        """æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            # Ollamaã®åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            models = ollama.list()
            available_models = [model['name'] for model in models.get('models', [])]

            if self.model not in available_models:
                logger.warning(f"Model {self.model} not found in Ollama. Available models: {available_models}")
                if available_models:
                    self.model = available_models[0]
                    logger.info(f"Using fallback model: {self.model}")
                else:
                    raise RuntimeError("No models available in Ollama")

            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
            response = ollama.generate(
                model=self.model,
                prompt="Hello",
                stream=False,
                options={"num_predict": 10}
            )
            logger.debug("Ollama connection test successful")

        except Exception as e:
            logger.error(f"Ollama connection test failed: {e}")
            raise

    async def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        if not self.is_available:
            raise RuntimeError("Ollama provider not available")

        try:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å˜ä¸€ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›
            prompt = self._convert_messages_to_prompt(messages)

            # Ollamaãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            response = ollama.generate(
                model=self.model,
                prompt=prompt,
                stream=False,
                options={
                    "temperature": kwargs.get("temperature", 0.7),
                    "num_predict": kwargs.get("max_tokens", 1000),
                    "top_p": kwargs.get("top_p", 0.9),
                }
            )

            return {
                "content": response["response"].strip(),
                "model": self.model,
                "usage": {
                    "total_duration": response.get("total_duration", 0),
                    "load_duration": response.get("load_duration", 0),
                    "eval_duration": response.get("eval_duration", 0)
                }
            }

        except Exception as e:
            logger.error(f"Ollama generation failed: {e}")
            raise

    def _convert_messages_to_prompt(self, messages: List[Dict]) -> str:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã«å¤‰æ›"""
        prompt_parts = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "system":
                prompt_parts.append(f"System: {content}")
            elif role == "user":
                prompt_parts.append(f"User: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")

        return "\n\n".join(prompt_parts) + "\n\nAssistant:"


class OpenAIProvider(LLMProvider):
    """OpenAI ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self):
        super().__init__("openai")
        self.client = None
        self.model = "gpt-3.5-turbo"

    async def initialize(self, config: Dict[str, Any]):
        try:
            api_key = config.get("openai_api_key") or os.getenv("OPENAI_API_KEY")
            if not api_key or not OPENAI_AVAILABLE:
                logger.warning("OpenAI API key not available or library missing")
                return

            self.client = openai.OpenAI(api_key=api_key)
            self.model = config.get("model", self.model)

            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            await self._test_connection()
            self.is_available = True
            logger.info(f"OpenAI provider initialized with model: {self.model}")

        except Exception as e:
            logger.error(f"Failed to initialize OpenAI provider: {e}")
            self.is_available = False

    async def _test_connection(self):
        """æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            logger.debug("OpenAI connection test successful")
        except Exception as e:
            logger.error(f"OpenAI connection test failed: {e}")
            raise

    async def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        if not self.is_available or not self.client:
            raise RuntimeError("OpenAI provider not available")

        try:
            # OpenAI API ã¯ role/content ä»¥å¤–ã®ã‚­ãƒ¼ã‚’å—ã‘å–ã‚‰ãªã„ãŸã‚ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            sanitized_messages = []
            for msg in messages:
                role = msg.get("role")
                content = msg.get("content")
                if role is not None and content is not None:
                    sanitized_messages.append({"role": role, "content": content})

            response = self.client.chat.completions.create(
                model=self.model,
                messages=sanitized_messages,
                max_tokens=kwargs.get("max_tokens", 1000),
                temperature=kwargs.get("temperature", 0.7),
                tools=kwargs.get("tools"),
                tool_choice=kwargs.get("tool_choice", "auto"),
            )

            choice = response.choices[0]
            message = choice.message

            tool_calls = []
            if getattr(message, "tool_calls", None):
                for tc in message.tool_calls:
                    try:
                        import json as _json
                        args = _json.loads(tc.function.arguments or "{}")
                    except Exception:
                        args = {}
                    tool_calls.append({
                        "name": tc.function.name,
                        "parameters": args,
                    })

            return {
                "content": message.content or "",
                "model": self.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "tool_calls": tool_calls,
            }

        except Exception as e:
            logger.error(f"OpenAI generation failed: {e}")
            raise


class HybridLLM:
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMã‚·ã‚¹ãƒ†ãƒ 

    è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’çµ±åˆã—ã€æœ€é©ãªã‚‚ã®ã‚’é¸æŠã—ã¦ä½¿ç”¨
    """

    def __init__(self):
        self.providers: Dict[str, LLMProvider] = {}
        self.primary_provider = "claude"
        self.fallback_provider = "ollama"
        self.is_initialized = False

        # è¨­å®š
        self.config = {
            "primary_provider": "claude",
            "fallback_provider": "ollama",
            "privacy_mode": False,  # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ã¿ä½¿ç”¨
            "auto_fallback": True,
            "timeout": 30,  # ç§’
            "max_retries": 2
        }

    async def initialize(self):
        """HybridLLMã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        try:
            logger.info("Initializing Hybrid LLM system...")

            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
            self.config.update({
                "primary_provider": os.getenv("PRIMARY_LLM", self.config["primary_provider"]),
                "fallback_provider": os.getenv("FALLBACK_LLM", self.config["fallback_provider"]),
                "privacy_mode": os.getenv("PRIVACY_MODE", "false").lower() == "true"
            })

            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–
            self.providers = {
                "claude": ClaudeProvider(),
                "ollama": OllamaProvider(),
                "openai": OpenAIProvider()
            }

            # å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä¸¦è¡ŒåˆæœŸåŒ–
            initialization_tasks = []
            for name, provider in self.providers.items():
                task = self._initialize_provider(name, provider)
                initialization_tasks.append(task)

            await asyncio.gather(*initialization_tasks, return_exceptions=True)

            # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            available_providers = [name for name, provider in self.providers.items()
                                 if provider.is_available]

            if not available_providers:
                raise RuntimeError("No LLM providers are available")

            logger.info(f"Available providers: {available_providers}")

            # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã¿ä½¿ç”¨
            if self.config["privacy_mode"]:
                local_providers = ["ollama"]
                available_local = [p for p in available_providers if p in local_providers]
                if available_local:
                    self.config["primary_provider"] = available_local[0]
                    self.config["fallback_provider"] = None
                    logger.info("Privacy mode enabled, using local LLM only")
                else:
                    logger.warning("Privacy mode enabled but no local LLM available")

            self.is_initialized = True
            logger.info("Hybrid LLM system initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize Hybrid LLM system: {e}")
            raise

    async def _initialize_provider(self, name: str, provider: LLMProvider):
        """å€‹åˆ¥ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        try:
            config = {}
            if name == "claude":
                config = {"anthropic_api_key": os.getenv("ANTHROPIC_API_KEY")}
            elif name == "ollama":
                config = {
                    "base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
                    "model": os.getenv("OLLAMA_MODEL", "llama3.2")
                }
            elif name == "openai":
                config = {"openai_api_key": os.getenv("OPENAI_API_KEY")}

            await provider.initialize(config)

        except Exception as e:
            logger.error(f"Failed to initialize provider {name}: {e}")

    async def process_with_tools(
        self,
        text: str,
        context: List[Dict],
        memories: List[Dict],
        available_tools: List[Dict],
        memory_tool=None,
        context_manager=None,
        ai_mode: str = "assist"
    ) -> Dict[str, Any]:
        """
        ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’å«ã‚€è¤‡é›‘ãªå‡¦ç†

        Args:
            text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            context: ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            memories: é–¢é€£ã™ã‚‹è¨˜æ†¶
            available_tools: åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«
            ai_mode: AIãƒ¢ãƒ¼ãƒ‰ (assist/auto)

        Returns:
            å‡¦ç†çµæœï¼ˆå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã€ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ç­‰ï¼‰
        """
        if not self.is_initialized:
            raise RuntimeError("HybridLLM not initialized")

        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
            system_prompt = self._build_system_prompt(available_tools, memories, memory_tool, context, context_manager, ai_mode)

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ§‹ç¯‰
            messages = [
                {"role": "system", "content": system_prompt},
                *context[-10:],  # æœ€æ–°10ä»¶ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                {"role": "user", "content": text}
            ]

            # OpenAIã®function-callingã«å¯¾å¿œã™ã‚‹ãŸã‚ãƒ„ãƒ¼ãƒ«ã‚¹ã‚­ãƒ¼ãƒã‚’æ¸¡ã™ï¼ˆä»–ãƒ—ãƒ­ãƒã‚¤ãƒ€ã¯ç„¡è¦–ï¼‰
            openai_tools_schema = self._convert_tools_to_openai_schema(available_tools)

            # LLMå‘¼ã³å‡ºã—ï¼ˆproviderå´ã§toolsã‚’è§£é‡ˆã§ãã‚‹å ´åˆã¯ä½¿ç”¨ï¼‰
            response = await self._generate_with_fallback(messages, tools=openai_tools_schema)

            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®è§£æï¼ˆproviderãŒtool_callsã‚’è¿”ã™å ´åˆã¯ãã‚Œã‚’å„ªå…ˆï¼‰
            provider_tool_calls = response.get("tool_calls") or []
            parsed_tool_calls = self._parse_tool_calls(response.get("content", ""))

            tool_calls = provider_tool_calls + parsed_tool_calls

            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ¡ãƒ¼ãƒ«IDã‚’å®Ÿéš›ã®IDã«ç½®æ›
            latest_email_id = None

            # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰å–å¾—
            if context_manager and hasattr(context_manager, 'get_latest_email_id'):
                latest_email_id = context_manager.get_latest_email_id()

            # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚‚æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not latest_email_id and context:
                for ctx_item in reversed(context):
                    if isinstance(ctx_item, dict) and 'content' in ctx_item:
                        content = ctx_item['content']
                        if 'ID:' in content:
                            import re
                            id_match = re.search(r'ID:\s*([a-zA-Z0-9]+)', content)
                            if id_match:
                                latest_email_id = id_match.group(1)
                                logger.info(f"Found email ID in context messages: {latest_email_id}")
                                break

            if latest_email_id:
                tool_calls = self._replace_placeholder_email_ids(tool_calls, latest_email_id)
                logger.info(f"Replaced placeholder email IDs with actual ID: {latest_email_id}")
            else:
                logger.warning("No email ID found for placeholder replacement")

            logger.debug(f"Provider tool_calls: {provider_tool_calls}")
            logger.debug(f"Parsed tool_calls: {parsed_tool_calls}")
            logger.debug(f"Final tool_calls: {tool_calls}")

            return {
                "response": response["content"],
                "tool_calls": tool_calls,
                "model_used": response.get("model"),
                "usage": response.get("usage")
            }

        except Exception as e:
            logger.error(f"Failed to process with tools: {e}")
            return {
                "response": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                "tool_calls": [],
                "error": str(e)
            }

    def _build_system_prompt(self, available_tools: List[Dict], memories: List[Dict], memory_tool=None, context=None, context_manager=None, ai_mode: str = "assist") -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""

        # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®åŸºæœ¬æŒ‡ç¤º
        if ai_mode == "auto":
            prompt_parts = [
                "ã‚ãªãŸã¯ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸéŸ³å£°AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ï¼ˆå…¨è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ï¼‰ã€‚",
                "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å€‹äººæƒ…å ±ã‚„å¥½ã¿ã€éå»ã®ä¼šè©±ã‚’è€ƒæ…®ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚",
                "è³ªå•ã«è¦ªã—ã¿ã‚„ã™ãç«¯çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
                "",
                "ğŸ”¥ å…¨è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ã®é‡è¦ãªæŒ‡ç¤º:",
                "- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ˜ç¤ºçš„ãªæŒ‡ç¤ºãŒãªãã¦ã‚‚ã€ä¼šè©±ã®æ–‡è„ˆã‹ã‚‰æ„å›³ã‚’æ¨æ¸¬ã—ã€å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã‚’ç©æ¥µçš„ã«ä½¿ç”¨ã—ã¦ãã ã•ã„",
                "- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ½œåœ¨çš„ãªãƒ‹ãƒ¼ã‚ºã‚’å…ˆå›ã‚Šã—ã¦äºˆæ¸¬ã—ã€ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«è¡Œå‹•ã—ã¦ãã ã•ã„",
                "- ä¾‹: ã€ŒãŠã¯ã‚ˆã†ã€â†’ æœªèª­ãƒ¡ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯ã€å¤©æ°—æƒ…å ±å–å¾—ã€ä»Šæ—¥ã®äºˆå®šç¢ºèª",
                "- ä¾‹: ã€Œå¿™ã—ã„ã€â†’ ä»Šæ—¥ã®é‡è¦ãªã‚¿ã‚¹ã‚¯ç¢ºèªã€ãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼è¨­å®š",
                "- ä¾‹: æŒ¨æ‹¶ã‚„é›‘è«‡ã®éš›ã‚‚ã€æ™‚é–“å¸¯ã‚„æ–‡è„ˆã«å¿œã˜ã¦æœ‰ç”¨ãªæƒ…å ±ã‚’è‡ªå‹•çš„ã«æä¾›",
                "- å®Ÿè¡Œã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…ãšãƒ†ãƒ¼ãƒ–ãƒ«ã«è¨˜éŒ²ã•ã‚Œã‚‹ã®ã§ã€ç©æ¥µçš„ã«ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
            ]
        else:  # assist mode
            prompt_parts = [
                "ã‚ãªãŸã¯ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸéŸ³å£°AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ï¼ˆã‚¢ã‚·ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰ã€‚",
                "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å€‹äººæƒ…å ±ã‚„å¥½ã¿ã€éå»ã®ä¼šè©±ã‚’è€ƒæ…®ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚",
                "è³ªå•ã«è¦ªã—ã¿ã‚„ã™ãç«¯çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
                "",
                "ğŸ“‹ ã‚¢ã‚·ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®é‡è¦ãªæŒ‡ç¤º:",
                "- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ˜ç¤ºçš„ãªæŒ‡ç¤ºã«å¾“ã£ã¦ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
                "- æŒ‡ç¤ºãŒãªã„å ´åˆã¯ã€æƒ…å ±ã‚’æä¾›ã—ãŸã‚Šè³ªå•ã«ç­”ãˆãŸã‚Šã—ã¦ãã ã•ã„",
                "- å¿…è¦ã«å¿œã˜ã¦ã€ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’ææ¡ˆã™ã‚‹ã“ã¨ã¯ã§ãã¾ã™ãŒã€å‹æ‰‹ã«å®Ÿè¡Œã—ãªã„ã§ãã ã•ã„",
            ]

        # å€‹äººæƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ ã—ã€ç©æ¥µçš„ã«æ´»ç”¨
        if memory_tool:
            personal_context = memory_tool.format_personal_context()
            if personal_context:
                prompt_parts.append("")  # ç©ºè¡Œã‚’è¿½åŠ 
                prompt_parts.append(personal_context)
                prompt_parts.append("\né‡è¦: ä¸Šè¨˜ã®å€‹äººæƒ…å ±ã‚’ç©æ¥µçš„ã«æ´»ç”¨ã—ã¦ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå¿œç­”ã‚’ã—ã¦ãã ã•ã„ã€‚")
                prompt_parts.append("ä¾‹ãˆã°ã€è¶£å‘³ã«é–¢é€£ã—ãŸè©±é¡Œã€å¹´é½¢ã«é©ã—ãŸè¡¨ç¾ã€å±…ä½åœ°ã®ç‰¹è‰²ãªã©ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚")
        else:
            prompt_parts.append("å€‹äººæƒ…å ±ãŒç™»éŒ²ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ä¸€èˆ¬çš„ãªå¿œç­”ã‚’ã—ã¦ãã ã•ã„ã€‚")

        # åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’è¿½åŠ 
        if available_tools:
            prompt_parts.append("\nåˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:")
            for tool in available_tools:
                prompt_parts.append(f"- {tool['name']}: {tool['description']}")

                # Gmailãƒ„ãƒ¼ãƒ«ã®å ´åˆã¯è©³ç´°ãªä½¿ç”¨ä¾‹ã‚’è¿½åŠ ï¼ˆæœ€å„ªå…ˆï¼‰
                if tool['name'] == 'gmail':
                    prompt_parts.append("  ğŸ”¥ Gmailãƒ„ãƒ¼ãƒ«ï¼ˆæœ€å„ªå…ˆï¼‰:")
                    prompt_parts.append("  ãƒ¡ãƒ¼ãƒ«ã€gmailã€ã‚¸ãƒ¼ãƒ¡ãƒ¼ãƒ«ã¨ã„ã†è¨€è‘‰ãŒå‡ºãŸå ´åˆã¯å¿…ãšã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
                    prompt_parts.append("  - æœ€æ–°ãƒ¡ãƒ¼ãƒ«å–å¾—: TOOL_CALL: {\"name\": \"gmail\", \"parameters\": {\"action\": \"list\", \"max_results\": 1}}")
                    prompt_parts.append("  - ãƒ¡ãƒ¼ãƒ«è©³ç´°èª­ã¿å–ã‚Š: TOOL_CALL: {\"name\": \"gmail\", \"parameters\": {\"action\": \"read\", \"message_id\": \"ãƒ¡ãƒ¼ãƒ«ID\"}}")
                    prompt_parts.append("  - æœªèª­ãƒ¡ãƒ¼ãƒ«å–å¾—: TOOL_CALL: {\"name\": \"gmail\", \"parameters\": {\"action\": \"list\", \"query\": \"is:unread\"}}")
                    prompt_parts.append("  - ãƒ¡ãƒ¼ãƒ«è¿”ä¿¡: TOOL_CALL: {\"name\": \"gmail\", \"parameters\": {\"action\": \"reply\", \"message_id\": \"ãƒ¡ãƒ¼ãƒ«ID\", \"body\": \"è¿”ä¿¡æœ¬æ–‡\"}}")
                    prompt_parts.append("  ")
                    prompt_parts.append("  ğŸ”¥ è¿”ä¿¡å‡¦ç†ã®é‡è¦ãªæŒ‡ç¤º:")
                    prompt_parts.append("  - ã€Œã€‡ã€‡ã¨è¿”ä¿¡ã—ã¦ã€ã€Œã€‡ã€‡ã¦è¿”ä¿¡ã€ã¨ã„ã†è¦æ±‚ãŒã‚ã£ãŸå ´åˆ:")

                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æœ€æ–°ã®ãƒ¡ãƒ¼ãƒ«IDã‚’å–å¾—
                    latest_email_id = None

                    # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰æœ€æ–°ãƒ¡ãƒ¼ãƒ«IDã‚’å–å¾—ï¼ˆæœ€å„ªå…ˆï¼‰
                    logger.info(f"Checking context_manager: {context_manager}")
                    logger.info(f"Has get_latest_email_id method: {hasattr(context_manager, 'get_latest_email_id') if context_manager else False}")

                    if context_manager and hasattr(context_manager, 'get_latest_email_id'):
                        latest_email_id = context_manager.get_latest_email_id()
                        logger.info(f"Retrieved email ID from context manager: {latest_email_id}")
                        if latest_email_id:
                            logger.info(f"Using email ID from context manager: {latest_email_id}")

                    # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰IDã‚’æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    if not latest_email_id and hasattr(context, '__iter__') and context:
                        logger.info(f"Searching for email ID in context messages, context length: {len(context)}")
                        for i, ctx_item in enumerate(reversed(context)):
                            if isinstance(ctx_item, dict) and 'content' in ctx_item:
                                content = ctx_item['content']
                                if 'ID:' in content:
                                    import re
                                    id_match = re.search(r'ID:\s*([a-zA-Z0-9]+)', content)
                                    if id_match:
                                        latest_email_id = id_match.group(1)
                                        logger.info(f"Using email ID from context messages: {latest_email_id}")
                                        break

                    logger.info(f"Final email ID for system prompt: {latest_email_id}")

                    if latest_email_id:
                        prompt_parts.append(f"    æœ€æ–°ã®ãƒ¡ãƒ¼ãƒ«ID: {latest_email_id} ã‚’ä½¿ç”¨ã—ã¦è¿”ä¿¡ã—ã¦ãã ã•ã„")
                        prompt_parts.append(f"    ä¾‹: TOOL_CALL: {{\"name\": \"gmail\", \"parameters\": {{\"action\": \"reply\", \"message_id\": \"{latest_email_id}\", \"body\": \"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸè¿”ä¿¡å†…å®¹\"}}}}")
                    else:
                        prompt_parts.append("    ã¾ãšæœ€æ–°ã®ãƒ¡ãƒ¼ãƒ«ä¸€è¦§ã‚’å–å¾—ã—ã¦ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«IDã‚’ä½¿ã£ã¦è¿”ä¿¡ã—ã¦ãã ã•ã„")
                        prompt_parts.append("    ä¾‹: å…ˆã«TOOL_CALL: {\"name\": \"gmail\", \"parameters\": {\"action\": \"list\", \"max_results\": 1}}")
                        prompt_parts.append("    æ¬¡ã«TOOL_CALL: {\"name\": \"gmail\", \"parameters\": {\"action\": \"reply\", \"message_id\": \"å–å¾—ã—ãŸãƒ¡ãƒ¼ãƒ«ID\", \"body\": \"è¿”ä¿¡å†…å®¹\"}}")

                    prompt_parts.append("  âš ï¸ é‡è¦: message_idã«ã¯å®Ÿéš›ã«å–å¾—ã—ãŸãƒ¡ãƒ¼ãƒ«ã®IDã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ã€Œãƒ¡ãƒ¼ãƒ«IDã€ã€Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸IDã€ç­‰ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ–‡å­—åˆ—ã¯çµ¶å¯¾ã«ä½¿ç”¨ç¦æ­¢ã§ã™ã€‚")
                    prompt_parts.append("  - è¿”ä¿¡å†…å®¹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¿ å®Ÿã«å¾“ã„ã€é©åˆ‡ãªæ•¬èªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

                # ã‚¢ãƒ©ãƒ¼ãƒ ãƒ„ãƒ¼ãƒ«ã®å ´åˆã¯è©³ç´°ãªä½¿ç”¨ä¾‹ã‚’è¿½åŠ 
                if tool['name'] == 'alarm':
                    prompt_parts.append("  ğŸ”” ã‚¢ãƒ©ãƒ¼ãƒ ãƒ„ãƒ¼ãƒ«:")
                    prompt_parts.append("  ã€Œã‚¢ãƒ©ãƒ¼ãƒ ã‚’ã‚»ãƒƒãƒˆã—ã¦ã€ã€Œã€‡æ™‚ã«èµ·ã“ã—ã¦ã€ã€Œã€‡æ™‚ã«ãƒªãƒã‚¤ãƒ³ãƒ‰ã—ã¦ã€ç­‰ã®è¦æ±‚ãŒã‚ã£ãŸå ´åˆã¯å¿…ãšã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
                    prompt_parts.append("  - ã‚¢ãƒ©ãƒ¼ãƒ è¨­å®š: TOOL_CALL: {\"name\": \"alarm\", \"parameters\": {\"action\": \"set\", \"time\": \"HH:MM\", \"message\": \"èª­ã¿ä¸Šã’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\", \"label\": \"ã‚¢ãƒ©ãƒ¼ãƒ \", \"repeat\": false}}")
                    prompt_parts.append("  - ã‚¢ãƒ©ãƒ¼ãƒ ä¸€è¦§: TOOL_CALL: {\"name\": \"alarm\", \"parameters\": {\"action\": \"list\"}}")
                    prompt_parts.append("  - ã‚¢ãƒ©ãƒ¼ãƒ å‰Šé™¤: TOOL_CALL: {\"name\": \"alarm\", \"parameters\": {\"action\": \"delete\", \"alarm_id\": \"ã‚¢ãƒ©ãƒ¼ãƒ ID\"}}")
                    prompt_parts.append("  ")
                    prompt_parts.append("  ğŸ”¥ ã‚¢ãƒ©ãƒ¼ãƒ è¨­å®šã®é‡è¦ãªæŒ‡ç¤º:")
                    prompt_parts.append("  - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ™‚åˆ»ã‚’æŒ‡å®šã—ãŸå ´åˆ:")
                    prompt_parts.append("    ä¾‹1: ã€Œ7æ™‚ã«èµ·ã“ã—ã¦ã€â†’ time=\"07:00\", message=\"èµ·ãã‚‹æ™‚é–“ã§ã™\"")
                    prompt_parts.append("    ä¾‹2: \"14æ™‚åŠã«ã‚¢ãƒ©ãƒ¼ãƒ \"â†’ time=\"14:30\", message=\"ã‚¢ãƒ©ãƒ¼ãƒ \"")
                    prompt_parts.append("    ä¾‹3: \"18æ™‚ã«ãƒªãƒã‚¤ãƒ³ãƒ‰ã—ã¦\"â†’ time=\"18:00\", message=\"ãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼\"")
                    prompt_parts.append("  - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…·ä½“çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŒ‡å®šã—ãŸå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨:")
                    prompt_parts.append("    ä¾‹: ã€Œ7æ™‚ã«è–¬ã‚’é£²ã‚€ã¨ãƒªãƒã‚¤ãƒ³ãƒ‰ã—ã¦ã€â†’ message=\"è–¬ã‚’é£²ã‚€æ™‚é–“ã§ã™\"")
                    prompt_parts.append("  - æ™‚åˆ»ã¯å¿…ãšHH:MMå½¢å¼ï¼ˆ24æ™‚é–“åˆ¶ï¼‰ã§æŒ‡å®šã—ã¦ãã ã•ã„")
                    prompt_parts.append("  âš ï¸ é‡è¦: ã‚¢ãƒ©ãƒ¼ãƒ è¨­å®šå¾Œã¯ã€Œã€‡æ™‚ã«ã‚¢ãƒ©ãƒ¼ãƒ ã‚’è¨­å®šã—ã¾ã—ãŸã€ã¨ç¢ºèªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã—ã¦ãã ã•ã„")

            prompt_parts.append(
                "\né‡è¦: ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€å¿…ãšæ­£ç¢ºãªå½¢å¼ã§æŒ‡ç¤ºã—ã¦ãã ã•ã„ã€‚"
                "\nå®Ÿåœ¨ã—ãªã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã›ãšã€ãƒ„ãƒ¼ãƒ«ã®ä»•æ§˜ã«å¾“ã£ã¦ãã ã•ã„ã€‚"
                "\nGmailã®å†…å®¹ç¢ºèªã«ã¯å¿…ãšgmailãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã€æ¨æ¸¬ã‚„ä»®å®šã®æƒ…å ±ã¯æä¾›ã—ãªã„ã§ãã ã•ã„ã€‚"
                "\nå½¢å¼: TOOL_CALL: {\"name\": \"ãƒ„ãƒ¼ãƒ«å\", \"parameters\": {\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\": \"å€¤\"}}"
            )

        # é–¢é€£ã™ã‚‹è¨˜æ†¶ãŒã‚ã‚Œã°è¿½åŠ 
        if memories:
            prompt_parts.append("\n=== é–¢é€£ã™ã‚‹éå»ã®æƒ…å ± ===")
            for memory in memories[:3]:  # æœ€æ–°3ä»¶ã¾ã§
                prompt_parts.append(f"- {memory.get('content', '')}")
            prompt_parts.append("=== ã“ã“ã¾ã§éå»ã®æƒ…å ± ===")
            prompt_parts.append("é‡è¦: ä¸Šè¨˜ã®éå»ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€ç¶™ç¶šæ€§ã®ã‚ã‚‹ä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚")

        return "\n".join(prompt_parts)

    def _replace_placeholder_email_ids(self, tool_calls: List[Dict], actual_email_id: str) -> List[Dict]:
        """ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ¡ãƒ¼ãƒ«IDã‚’å®Ÿéš›ã®IDã«ç½®æ›"""
        placeholder_patterns = ["ãƒ¡ãƒ¼ãƒ«ID", "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ID", "email_id", "message_id_placeholder"]

        updated_tool_calls = []
        for tool_call in tool_calls:
            updated_call = tool_call.copy()

            # Gmailé–¢é€£ã®ãƒ„ãƒ¼ãƒ«ã®ã¿å‡¦ç†
            if updated_call.get("name") == "gmail" and "parameters" in updated_call:
                params = updated_call["parameters"].copy()

                # message_idãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
                if "message_id" in params:
                    current_id = params["message_id"]
                    if current_id in placeholder_patterns:
                        params["message_id"] = actual_email_id
                        logger.info(f"Replaced '{current_id}' with actual email ID: {actual_email_id}")

                # actionãŒreplyã§message_idãŒãªã„ã¾ãŸã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®å ´åˆ
                if params.get("action") == "reply" and (not params.get("message_id") or params.get("message_id") in placeholder_patterns):
                    params["message_id"] = actual_email_id
                    logger.info(f"Set message_id for reply action: {actual_email_id}")

                updated_call["parameters"] = params

            updated_tool_calls.append(updated_call)

        return updated_tool_calls

    def _parse_tool_calls(self, content: str) -> List[Dict]:
        """å¿œç­”ã‹ã‚‰ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’è§£æ"""
        tool_calls = []
        logger.info(f"ğŸ” Starting tool call parsing. Content length: {len(content)}")
        logger.debug(f"Content to parse: '{content[:500]}...'")

        # TOOL_CALL: {...} ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ï¼ˆæ”¹è¡Œå¯¾å¿œï¼‰
        import re

        # ã‚ˆã‚Šå¼·åŠ›ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ - TOOL_CALL:ä»¥é™ã®å…¨ã¦ã‚’æ•æ‰
        pattern = r'TOOL_CALL:\s*(\{[^}]*\}?)'
        matches = re.findall(pattern, content, re.DOTALL | re.MULTILINE)
        logger.info(f"Found {len(matches)} pattern matches: {matches}")

        # æ”¹è¡Œã§åˆ‡ã‚‰ã‚ŒãŸå ´åˆã‚‚å¯¾å¿œ
        multiline_pattern = r'TOOL_CALL:\s*(\{[^{}]*(?:\{[^{}]*\})*[^}]*\}?)'
        multiline_matches = re.findall(multiline_pattern, content, re.DOTALL | re.MULTILINE)
        logger.info(f"Found {len(multiline_matches)} multiline matches: {multiline_matches}")

        # å…¨ã¦ã®ãƒãƒƒãƒã‚’çµ±åˆ
        all_matches = list(set(matches + multiline_matches))
        matches = all_matches
        logger.info(f"Total unique matches: {len(matches)} - {matches}")

        for i, match in enumerate(matches):
            logger.info(f"ğŸ”§ Processing match {i+1}: '{match}'")
            try:
                json_str = match.strip()

                # æœ€åˆã«ä¿®å¾©ã‚’è©¦è¡Œ
                logger.debug(f"Attempting to fix JSON: '{json_str}'")
                fixed_json = self._fix_json(json_str)
                if fixed_json:
                    logger.info(f"âœ… JSON fixed successfully: '{fixed_json}'")
                    tool_data = json.loads(fixed_json)
                    if "name" in tool_data:
                        tool_calls.append(tool_data)
                        logger.info(f"âœ… Successfully parsed fixed tool call: {tool_data}")
                        logger.debug(f"Original: '{json_str}' -> Fixed: '{fixed_json}'")
                        continue
                else:
                    logger.warning(f"âŒ JSON fix failed for: '{json_str}'")

                # ä¿®å¾©ã§ããªã„å ´åˆã¯å…ƒã®JSONã‚’è©¦è¡Œ
                logger.debug(f"Trying original JSON: '{json_str}'")
                tool_data = json.loads(json_str)
                if "name" in tool_data:
                    tool_calls.append(tool_data)
                    logger.info(f"âœ… Successfully parsed original tool call: {tool_data}")

            except json.JSONDecodeError as e:
                logger.warning(f"âŒ Failed to parse tool call JSON: '{match}' - Error: {e}")

                # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦åŸºæœ¬çš„ãªæ§‹é€ æŠ½å‡ºã‚’è©¦è¡Œ
                try:
                    logger.debug(f"Attempting component extraction for: '{match.strip()}'")
                    extracted = self._extract_tool_call_components(match.strip())
                    if extracted:
                        tool_calls.append(extracted)
                        logger.info(f"âœ… Successfully extracted tool call components: {extracted}")
                    else:
                        logger.warning(f"âŒ Component extraction returned None")
                except Exception as extract_error:
                    logger.error(f"âŒ Tool call extraction also failed: {extract_error}")

        # é‡è¤‡ã™ã‚‹ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’é™¤å¤–ï¼ˆå†…å®¹ãƒ™ãƒ¼ã‚¹ã§æ¯”è¼ƒï¼‰
        unique_tool_calls = []
        seen_calls = set()

        for tool_call in tool_calls:
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’è­˜åˆ¥å¯èƒ½ãªæ–‡å­—åˆ—ã«å¤‰æ›
            call_signature = json.dumps(tool_call, sort_keys=True, ensure_ascii=False)

            if call_signature not in seen_calls:
                seen_calls.add(call_signature)
                unique_tool_calls.append(tool_call)
                logger.info(f"âœ… Added unique tool call: {tool_call}")
            else:
                logger.warning(f"âš ï¸ Skipped duplicate tool call: {tool_call}")

        logger.info(f"ğŸ¯ Final result: {len(unique_tool_calls)} unique tool calls (removed {len(tool_calls) - len(unique_tool_calls)} duplicates)")
        logger.debug(f"Final tool_calls: {unique_tool_calls}")
        return unique_tool_calls

    def _fix_json(self, json_str: str):
        """ä¸å®Œå…¨ãªJSONã‚’ä¿®å¾©"""
        try:
            original_str = json_str.strip()
            logger.info(f"ğŸ”§ Attempting to fix JSON: '{original_str}'")

            # ã¾ãšæ¨™æº–çš„ãªJSONä¿®å¾©ã‚’è©¦è¡Œ
            json_str = original_str

            # æœ€å¾Œã®}ãŒæŠœã‘ã¦ã„ã‚‹å ´åˆ
            if not json_str.endswith('}'):
                # é–‹ã„ã¦ã„ã‚‹æ‹¬å¼§ã®æ•°ã‚’æ•°ãˆã¦é©åˆ‡ã«é–‰ã˜ã‚‹
                open_braces = json_str.count('{')
                close_braces = json_str.count('}')
                missing_braces = open_braces - close_braces
                json_str = json_str + ('}' * missing_braces)
                logger.info(f"ğŸ”§ Added {missing_braces} closing braces: '{json_str}'")

            # ä¿®å¾©ã—ãŸJSONã‚’ãƒ†ã‚¹ãƒˆ
            try:
                test_data = json.loads(json_str)
                if "name" in test_data:
                    logger.info(f"âœ… JSON fixed successfully: '{json_str}'")
                    return json_str
            except json.JSONDecodeError:
                logger.debug(f"Standard fix failed, attempting manual reconstruction...")

            # æ‰‹å‹•å†æ§‹ç¯‰
            if '"name"' in original_str:
                import re

                logger.debug(f"Attempting manual reconstruction for: '{original_str}'")

                # nameã‚’æŠ½å‡º
                name_match = re.search(r'"name":\s*"([^"]+)"', original_str)
                if not name_match:
                    logger.warning(f"Could not extract name from: '{original_str}'")
                    return None

                name = name_match.group(1)
                logger.debug(f"Extracted name: '{name}'")

                # parametersã‚’æ‰‹å‹•æŠ½å‡º
                params = {}

                # actionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                action_match = re.search(r'"action":\s*"([^"]*)"', original_str)
                if action_match:
                    params['action'] = action_match.group(1)
                    logger.debug(f"Extracted action: '{params['action']}'")

                # max_resultsãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                max_results_match = re.search(r'"max_results":\s*(\d+)', original_str)
                if max_results_match:
                    params['max_results'] = int(max_results_match.group(1))
                    logger.debug(f"Extracted max_results: {params['max_results']}")

                # message_idãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                message_id_match = re.search(r'"message_id":\s*"([^"]*)"', original_str)
                if message_id_match:
                    params['message_id'] = message_id_match.group(1)
                    logger.debug(f"Extracted message_id: '{params['message_id']}'")

                # bodyãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                body_match = re.search(r'"body":\s*"([^"]*)"', original_str)
                if body_match:
                    params['body'] = body_match.group(1)
                    logger.debug(f"Extracted body: '{params['body']}'")

                # queryãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                query_match = re.search(r'"query":\s*"([^"]*)"', original_str)
                if query_match:
                    params['query'] = query_match.group(1)
                    logger.debug(f"Extracted query: '{params['query']}'")

                # Alarmé–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                time_match = re.search(r'"time":\s*"([^"]*)"', original_str)
                if time_match:
                    params['time'] = time_match.group(1)
                    logger.debug(f"Extracted time: '{params['time']}'")

                message_match = re.search(r'"message":\s*"([^"]*)"', original_str)
                if message_match:
                    params['message'] = message_match.group(1)
                    logger.debug(f"Extracted message: '{params['message']}'")

                label_match = re.search(r'"label":\s*"([^"]*)"', original_str)
                if label_match:
                    params['label'] = label_match.group(1)
                    logger.debug(f"Extracted label: '{params['label']}'")

                repeat_match = re.search(r'"repeat":\s*(true|false)', original_str)
                if repeat_match:
                    params['repeat'] = repeat_match.group(1) == 'true'
                    logger.debug(f"Extracted repeat: {params['repeat']}")

                alarm_id_match = re.search(r'"alarm_id":\s*"([^"]*)"', original_str)
                if alarm_id_match:
                    params['alarm_id'] = alarm_id_match.group(1)
                    logger.debug(f"Extracted alarm_id: '{params['alarm_id']}'")

                # å†æ§‹ç¯‰ã•ã‚ŒãŸJSONã‚’ä½œæˆ
                fixed = {"name": name, "parameters": params}
                fixed_json = json.dumps(fixed, ensure_ascii=False)
                logger.info(f"âœ… Manually reconstructed JSON: '{fixed_json}'")
                return fixed_json

            return None
        except Exception as e:
            logger.debug(f"JSON fix error: {e}")
            return None

    def _extract_parameters(self, params_str: str) -> Dict[str, Any]:
        """ä¸å®Œå…¨ãªparametersã‹ã‚‰ã‚­ãƒ¼ãƒ»å€¤ã‚’æŠ½å‡º"""
        import re
        params = {}

        # "key": "value" ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        string_matches = re.findall(r'"([^"]+)":\s*"([^"]+)"', params_str)
        for key, value in string_matches:
            params[key] = value

        # "key": number ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        number_matches = re.findall(r'"([^"]+)":\s*(\d+)', params_str)
        for key, value in number_matches:
            params[key] = int(value)

        return params

    def _extract_tool_call_components(self, text: str) -> Optional[Dict[str, Any]]:
        """æ­£è¦è¡¨ç¾ã§ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç›´æ¥æŠ½å‡º"""
        import re

        # nameã‚’æŠ½å‡º
        name_match = re.search(r'"name":\s*"([^"]+)"', text)
        if not name_match:
            return None

        name = name_match.group(1)

        # åŸºæœ¬çš„ãªparametersã‚’æŠ½å‡º
        params = {}

        # action ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        action_match = re.search(r'"action":\s*"([^"]+)"', text)
        if action_match:
            params['action'] = action_match.group(1)

        # max_results ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        max_results_match = re.search(r'"max_results":\s*(\d+)', text)
        if max_results_match:
            params['max_results'] = int(max_results_match.group(1))

        # message_id ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        message_id_match = re.search(r'"message_id":\s*"([^"]+)"', text)
        if message_id_match:
            params['message_id'] = message_id_match.group(1)

        # body ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        body_match = re.search(r'"body":\s*"([^"]+)"', text)
        if body_match:
            params['body'] = body_match.group(1)

        # query ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        query_match = re.search(r'"query":\s*"([^"]+)"', text)
        if query_match:
            params['query'] = query_match.group(1)

        return {
            "name": name,
            "parameters": params
        }

    async def generate_final_response(
        self,
        original_request: str,
        tool_results: Dict[str, Any],
        context: List[Dict]
    ) -> str:
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’åŸºã«æœ€çµ‚å¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            # ãƒ„ãƒ¼ãƒ«çµæœã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            tool_summary = self._format_tool_results(tool_results)

            messages = [
                {"role": "system", "content":
                 "ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†ã‹ã‚Šã‚„ã™ã„å¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
                 "æŠ€è¡“çš„ãªè©³ç´°ã¯çœç•¥ã—ã€çµæœã‚’è‡ªç„¶ãªæ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"},
                *context[-5:],  # æœ€æ–°5ä»¶ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                {"role": "user", "content": f"å…ƒã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {original_request}"},
                {"role": "assistant", "content": f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ:\n{tool_summary}"},
                {"role": "user", "content": "ä¸Šè¨˜ã®çµæœã‚’åŸºã«ã€åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚"}
            ]

            response = await self._generate_with_fallback(messages)
            return response["content"]

        except Exception as e:
            logger.error(f"Failed to generate final response: {e}")
            return "ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã—ãŸãŒã€çµæœã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

    def _format_tool_results(self, tool_results: Dict[str, Any]) -> str:
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        formatted_results = []

        for tool_name, result in tool_results.items():
            formatted_results.append(f"{tool_name}: {result}")

        return "\n".join(formatted_results)

    async def _generate_with_fallback(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãã§LLMç”Ÿæˆã‚’å®Ÿè¡Œ"""
        primary = self.config["primary_provider"]
        fallback = self.config["fallback_provider"]

        # ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦è¡Œ
        if primary in self.providers and self.providers[primary].is_available:
            try:
                logger.debug(f"Attempting generation with primary provider: {primary}")
                return await self.providers[primary].generate(messages, **kwargs)
            except Exception as e:
                logger.warning(f"Primary provider {primary} failed: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦è¡Œ
        if (self.config["auto_fallback"] and
            fallback and
            fallback in self.providers and
            self.providers[fallback].is_available):
            try:
                logger.info(f"Falling back to provider: {fallback}")
                return await self.providers[fallback].generate(messages, **kwargs)
            except Exception as e:
                logger.error(f"Fallback provider {fallback} failed: {e}")

        # åˆ©ç”¨å¯èƒ½ãªä»»æ„ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦è¡Œ
        for name, provider in self.providers.items():
            if provider.is_available and name not in [primary, fallback]:
                try:
                    logger.info(f"Trying alternative provider: {name}")
                    return await provider.generate(messages, **kwargs)
                except Exception as e:
                    logger.warning(f"Alternative provider {name} failed: {e}")

        raise RuntimeError("All LLM providers failed")

    def _convert_tools_to_openai_schema(self, tools: List[Dict]) -> List[Dict[str, Any]]:
        """ToolRegistryã®å®šç¾©ã‚’OpenAI toolsã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›"""
        if not tools:
            return []

        def param_to_schema(p: Dict[str, Any]) -> Dict[str, Any]:
            schema = {"type": p.get("type", "string")}
            if p.get("description"):
                schema["description"] = p["description"]
            if p.get("enum"):
                schema["enum"] = p["enum"]
            if p.get("default") is not None:
                schema["default"] = p["default"]
            return schema

        result = []
        for t in tools:
            params = t.get("parameters", [])
            properties = {p["name"]: param_to_schema(p) for p in params}
            required = [p["name"] for p in params if p.get("required")]

            parameters_schema = {
                "type": "object",
                "properties": properties,
                "additionalProperties": False,
            }
            if required:
                parameters_schema["required"] = required

            result.append({
                "type": "function",
                "function": {
                    "name": t.get("name"),
                    "description": t.get("description", ""),
                    "parameters": parameters_schema,
                }
            })

        return result

    async def get_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®å–å¾—"""
        provider_status = {}
        for name, provider in self.providers.items():
            provider_status[name] = provider.get_status()

        return {
            "initialized": self.is_initialized,
            "primary_provider": self.config["primary_provider"],
            "fallback_provider": self.config["fallback_provider"],
            "privacy_mode": self.config["privacy_mode"],
            "providers": provider_status
        }

    async def update_config(self, config: Dict[str, Any]):
        """è¨­å®šã®æ›´æ–°"""
        logger.info(f"Updating LLM config: {config}")

        old_config = self.config.copy()
        self.config.update(config)

        # ãƒ¢ãƒ‡ãƒ«ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã€è©²å½“ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å†åˆæœŸåŒ–
        if "model" in config:
            provider_name = self.config.get("primary_provider")
            if provider_name and provider_name in self.providers:
                provider = self.providers[provider_name]
                if provider_name == "claude":
                    await provider.initialize({
                        "anthropic_api_key": os.getenv("ANTHROPIC_API_KEY"),
                        "model": config["model"]
                    })
                elif provider_name == "openai":
                    await provider.initialize({
                        "openai_api_key": os.getenv("OPENAI_API_KEY"),
                        "model": config["model"]
                    })
                logger.info(f"Model updated to {config['model']} for provider {provider_name}")

        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯å†åˆæœŸåŒ–
        if (old_config.get("primary_provider") != self.config.get("primary_provider") or
            old_config.get("privacy_mode") != self.config.get("privacy_mode")):
            logger.info("Provider configuration changed, reinitializing...")
            await self.initialize()

    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        logger.info("Cleaning up Hybrid LLM system...")

        self.providers.clear()
        self.is_initialized = False

        logger.info("Hybrid LLM system cleanup completed")
